use anyhow::Result;
use async_trait::async_trait;
use rig::client::CompletionClient;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;

use crate::agents::{
    core::{AgentBackend, AgentContext, TaskResult},
    tools::AgentTool,
};
use crate::commit::prompt::{
    create_pr_system_prompt, create_pr_user_prompt, create_review_system_prompt,
    create_review_user_prompt, create_system_prompt, create_user_prompt,
};
use crate::commit::review::GeneratedReview;
use crate::commit::types::{GeneratedMessage, GeneratedPullRequest};
use crate::context::CommitContext;
use crate::log_debug;
use crate::{
    iris_status_analysis, iris_status_completed, iris_status_expansion, iris_status_generation,
    iris_status_planning, iris_status_synthesis, iris_status_tool,
};

/// The unified Iris agent - an AI assistant for all Git-Iris operations
pub struct IrisAgent {
    id: String,
    name: String,
    description: String,
    capabilities: Vec<String>,
    backend: AgentBackend,
    tools: Vec<Arc<dyn AgentTool>>,
    knowledge: std::sync::Mutex<IrisKnowledge>,
    initialized: bool,
}

/// Intelligence context gathered through LLM analysis
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IntelligentContext {
    pub files_with_relevance: Vec<FileRelevance>,
    pub change_summary: String,
    pub technical_analysis: String,
    pub project_insights: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FileRelevance {
    pub path: String,
    pub relevance_score: f32,
    pub analysis: String,
    pub key_changes: Vec<String>,
    pub impact_assessment: String,
}

/// Plan for tool execution generated by the agent
#[derive(Debug, Clone)]
pub struct ToolPlan {
    pub tool_name: String,
    pub operation: Option<String>,
    pub parameters: std::collections::HashMap<String, serde_json::Value>,
    pub reason: String,
}

/// Result from tool execution
#[derive(Debug, Clone)]
pub struct ToolResult {
    pub tool_name: String,
    pub operation: Option<String>,
    pub result: serde_json::Value,
    pub reason: String,
}

/// Iris's knowledge base - notes taken during task execution
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IrisKnowledge {
    pub project_insights: Vec<String>,
    pub code_patterns: Vec<String>,
    pub discovered_issues: Vec<String>,
    pub architectural_notes: Vec<String>,
    pub performance_observations: Vec<String>,
    pub security_findings: Vec<String>,
    pub learned_context: Vec<String>,
    pub tool_effectiveness: HashMap<String, String>,
    pub execution_metadata: HashMap<String, serde_json::Value>,
}

impl Default for IrisKnowledge {
    fn default() -> Self {
        Self {
            project_insights: Vec::new(),
            code_patterns: Vec::new(),
            discovered_issues: Vec::new(),
            architectural_notes: Vec::new(),
            performance_observations: Vec::new(),
            security_findings: Vec::new(),
            learned_context: Vec::new(),
            tool_effectiveness: HashMap::new(),
            execution_metadata: HashMap::new(),
        }
    }
}

impl IrisKnowledge {
    /// Add a new insight to Iris's knowledge base
    pub fn add_insight(&mut self, category: &str, insight: String) {
        match category {
            "project" => self.project_insights.push(insight),
            "patterns" => self.code_patterns.push(insight),
            "issues" => self.discovered_issues.push(insight),
            "architecture" => self.architectural_notes.push(insight),
            "performance" => self.performance_observations.push(insight),
            "security" => self.security_findings.push(insight),
            "context" => self.learned_context.push(insight),
            _ => self.learned_context.push(format!("[{}] {}", category, insight)),
        }
    }

    /// Get a summary of current knowledge for context
    pub fn get_summary(&self) -> String {
        let mut summary = String::new();
        
        if !self.project_insights.is_empty() {
            summary.push_str("**Project Insights:**\n");
            for insight in &self.project_insights {
                summary.push_str(&format!("- {}\n", insight));
            }
            summary.push('\n');
        }
        
        if !self.code_patterns.is_empty() {
            summary.push_str("**Code Patterns Observed:**\n");
            for pattern in &self.code_patterns {
                summary.push_str(&format!("- {}\n", pattern));
            }
            summary.push('\n');
        }
        
        if !self.discovered_issues.is_empty() {
            summary.push_str("**Issues Discovered:**\n");
            for issue in &self.discovered_issues {
                summary.push_str(&format!("- {}\n", issue));
            }
            summary.push('\n');
        }
        
        if !self.security_findings.is_empty() {
            summary.push_str("**Security Findings:**\n");
            for finding in &self.security_findings {
                summary.push_str(&format!("- {}\n", finding));
            }
            summary.push('\n');
        }
        
        summary
    }
}

impl IrisAgent {
    pub fn new(backend: AgentBackend, tools: Vec<Arc<dyn AgentTool>>) -> Self {
        let mut agent = Self {
            id: "iris_agent".to_string(),
            name: "Iris".to_string(),
            description: "AI assistant for intelligent Git workflow automation and analysis"
                .to_string(),
            capabilities: vec![
                "commit_message_generation".to_string(),
                "code_review".to_string(),
                "pull_request_description".to_string(),
                "changelog_generation".to_string(),
                "file_analysis".to_string(),
                "relevance_scoring".to_string(),
                "intelligent_context_gathering".to_string(),
                "diff_analysis".to_string(),
                "change_summarization".to_string(),
                "commit_validation".to_string(),
                "security_analysis".to_string(),
                "performance_analysis".to_string(),
                "documentation_review".to_string(),
                "context_summarization".to_string(),
                "chunked_analysis".to_string(),
                "knowledge_building".to_string(),
                "adaptive_learning".to_string(),
            ],
            backend,
            tools,
            knowledge: std::sync::Mutex::new(IrisKnowledge::default()),
            initialized: false,
        };

        agent.initialized = true;
        agent
    }

    /// Add knowledge learned during task execution
    fn add_knowledge(&self, category: &str, insight: String) {
        if let Ok(mut knowledge) = self.knowledge.lock() {
            log_debug!("üß† Iris: Added {} insight: {}", category, insight);
            knowledge.add_insight(category, insight);
        }
    }

    /// Get current knowledge summary for context
    fn get_knowledge_context(&self) -> String {
        if let Ok(knowledge) = self.knowledge.lock() {
            knowledge.get_summary()
        } else {
            String::new()
        }
    }

    /// Ask LLM to extract insights from a result and add to knowledge base
    async fn learn_from_result(&self, task_type: &str, result: &str) -> Result<()> {
        let learning_prompt = format!(
            "You are Iris. Analyze this {} result and extract key insights for future tasks.

            Result:
            {}

            Extract insights in these categories (return only insights that are genuinely useful):
            1. **project**: General project architecture or patterns
            2. **patterns**: Code patterns, conventions, or styles observed
            3. **issues**: Common issues or anti-patterns found
            4. **architecture**: Architectural decisions or design patterns
            5. **performance**: Performance considerations or optimizations
            6. **security**: Security patterns or vulnerabilities
            7. **context**: Important context about this specific codebase

            Respond with JSON:
            {{
              \"insights\": [
                {{\"category\": \"project\", \"insight\": \"This project uses X pattern for Y\"}},
                {{\"category\": \"patterns\", \"insight\": \"Code follows Z convention\"}}
              ]
            }}

            Only include insights that would be helpful for future code analysis.",
            task_type, result
        );

        if let Ok(learning_response) = self.analyze_with_backend(&learning_prompt).await {
            if let Ok(insights) = self.parse_json_response::<serde_json::Value>(&learning_response).await {
                if let Some(insights_array) = insights.get("insights").and_then(|i| i.as_array()) {
                    for insight in insights_array {
                        if let (Some(category), Some(text)) = (
                            insight.get("category").and_then(|c| c.as_str()),
                            insight.get("insight").and_then(|i| i.as_str())
                        ) {
                            self.add_knowledge(category, text.to_string());
                        }
                    }
                }
            }
        }

        Ok(())
    }

    /// Use LLM to intelligently manage context for code reviews
    async fn manage_review_context(&self, context: &CommitContext) -> Result<String> {
        iris_status_analysis!();
        
        log_debug!("üß† Iris: Using LLM to intelligently manage review context");
        
        let full_context = create_review_user_prompt(context);
        let context_size = full_context.len();
        
        log_debug!("üìè Iris: Full context size: {} characters", context_size);
        
        // If context is reasonable size, use it directly
        if context_size < 8000 {
            log_debug!("‚úÖ Iris: Context size manageable, proceeding with full review");
            return Ok(full_context);
        }
        
        // Let LLM intelligently summarize and prioritize
        let smart_context_prompt = format!(
            "You are Iris, an expert code reviewer. The code context below is too large for optimal review. 
            
            Your task: Create a focused, intelligent summary that preserves all critical information needed for a comprehensive code review.
            
            **What to preserve:**
            - All security-critical changes
            - Complex logic that needs careful review  
            - Performance-sensitive code
            - Error handling patterns
            - API changes or breaking changes
            - Critical diff sections (keep exact code)
            
            **What to summarize:**
            - Repetitive patterns
            - Simple refactoring
            - Formatting changes
            - Non-critical utility functions
            
            **Original Context ({} chars):**
            {}
            
            Create an intelligent, focused review context that captures everything important while being concise enough for thorough analysis.",
            context_size,
            full_context
        );

        let managed_context = self.analyze_with_backend(&smart_context_prompt).await?;
        log_debug!("‚úÖ Iris: Created LLM-managed context - {} chars (reduced from {})", 
                   managed_context.len(), context_size);
        
        Ok(managed_context)
    }

    /// Generate a commit message with intelligent context analysis
    pub async fn generate_commit_message(
        &self,
        context: &AgentContext,
        params: &HashMap<String, serde_json::Value>,
    ) -> Result<TaskResult> {
        let preset = params
            .get("preset")
            .and_then(|v| v.as_str())
            .unwrap_or("default");
        let _custom_instructions = params
            .get("instructions")
            .and_then(|v| v.as_str())
            .unwrap_or("");
        let use_gitmoji = params
            .get("gitmoji")
            .and_then(serde_json::Value::as_bool)
            .unwrap_or(context.config.use_gitmoji);

        log_debug!(
            "ü§ñ Iris: Generating commit message with preset: '{}', gitmoji: {}",
            preset,
            use_gitmoji
        );

        // Step 1: Gather intelligent context using LLM analysis
        log_debug!("üß† Iris: Gathering intelligent context through LLM analysis");
        let intelligent_context = self.gather_intelligent_context(context).await?;

        // Step 2: Build Git context from intelligent analysis
        let commit_context = self
            .build_commit_context(context, &intelligent_context)
            .await?;

        // Step 3: Generate commit message using existing prompt system
        log_debug!("üìù Iris: Building system and user prompts");
        let system_prompt = create_system_prompt(&context.config)?;
        let user_prompt = create_user_prompt(&commit_context);

        log_debug!(
            "üìè Iris: Prompts built - System: {} chars, User: {} chars",
            system_prompt.len(),
            user_prompt.len()
        );

        // Step 4: Generate with LLM
        log_debug!("ü§ñ Iris: Generating commit message with LLM");
        iris_status_generation!();
        let generated_message = self
            .generate_with_backend(&system_prompt, &user_prompt)
            .await?;

        // Step 5: Parse and validate response
        let parsed_response = self
            .parse_json_response::<GeneratedMessage>(&generated_message)
            .await?;

        log_debug!(
            "‚úÖ Iris: Commit message generated - Title: '{}', {} chars total",
            parsed_response.title,
            parsed_response.message.len()
        );

        iris_status_completed!();
        Ok(TaskResult::success_with_data(
            "Commit message generated successfully".to_string(),
            serde_json::to_value(parsed_response)?,
        )
        .with_confidence(0.92))
    }

    /// Generate a code review with intelligent analysis
    pub async fn generate_code_review(
        &self,
        context: &AgentContext,
        _params: &HashMap<String, serde_json::Value>,
    ) -> Result<TaskResult> {
        log_debug!("üîç Iris: Starting intelligent code review");

        // Gather intelligent context
        let intelligent_context = self.gather_intelligent_context(context).await?;
        let commit_context = self
            .build_commit_context(context, &intelligent_context)
            .await?;

        // Generate review using existing prompt system with intelligent context management
        let system_prompt = create_review_system_prompt(&context.config)?;
        let managed_user_prompt = self.manage_review_context(&commit_context).await?;

        let generated_review = self
            .generate_with_backend(&system_prompt, &managed_user_prompt)
            .await?;
        let parsed_response = self
            .parse_json_response::<GeneratedReview>(&generated_review)
            .await?;

        log_debug!(
            "‚úÖ Iris: Code review completed with {} issues",
            parsed_response.issues.len()
        );

        Ok(TaskResult::success_with_data(
            "Code review generated successfully".to_string(),
            serde_json::to_value(parsed_response)?,
        )
        .with_confidence(0.88))
    }

    /// Generate a pull request description with intelligent analysis
    pub async fn generate_pull_request(
        &self,
        context: &AgentContext,
        params: &HashMap<String, serde_json::Value>,
    ) -> Result<TaskResult> {
        log_debug!("üìã Iris: Starting pull request description generation");

        // Get commit messages from params
        let commit_messages = params
            .get("commit_messages")
            .and_then(|v| v.as_array())
            .map(|arr| {
                arr.iter()
                    .filter_map(|v| v.as_str())
                    .map(std::string::ToString::to_string)
                    .collect::<Vec<_>>()
            })
            .unwrap_or_default();

        // Gather intelligent context
        let intelligent_context = self.gather_intelligent_context(context).await?;
        let commit_context = self
            .build_commit_context(context, &intelligent_context)
            .await?;

        // Generate PR description using existing prompt system
        let system_prompt = create_pr_system_prompt(&context.config)?;
        let user_prompt = create_pr_user_prompt(&commit_context, &commit_messages);

        let generated_pr = self
            .generate_with_backend(&system_prompt, &user_prompt)
            .await?;
        let parsed_response = self
            .parse_json_response::<GeneratedPullRequest>(&generated_pr)
            .await?;

        log_debug!(
            "‚úÖ Iris: PR description generated - Title: '{}'",
            parsed_response.title
        );

        Ok(TaskResult::success_with_data(
            "Pull request description generated successfully".to_string(),
            serde_json::to_value(parsed_response)?,
        )
        .with_confidence(0.90))
    }

    /// Gather intelligent context using LLM-driven tool selection and usage
    async fn gather_intelligent_context(
        &self,
        context: &AgentContext,
    ) -> Result<IntelligentContext> {
        log_debug!(
            "üß† Iris: Starting intelligent context analysis with agent-driven tool selection"
        );

        // Step 1: Let the agent decide which tools to use
        log_debug!("ü§ñ Iris: Planning tool usage strategy");
        iris_status_planning!();
        let tool_plan = self.plan_tool_usage_for_context_analysis(context).await?;
        log_debug!("üìã Iris: Agent planned {} tool operations", tool_plan.len());

        // Step 2: Execute the planned tool calls
        log_debug!("üîß Iris: Executing agent-planned tool calls");
        let tool_results = self.execute_planned_tool_calls(context, &tool_plan).await?;
        log_debug!("‚úÖ Iris: Completed {} tool operations", tool_results.len());

        // Step 3: Use LLM to synthesize tool results into intelligent context
        log_debug!("ü§ñ Iris: Synthesizing tool results with LLM");
        iris_status_synthesis!();
        let synthesis_prompt = self.build_tool_synthesis_prompt(&tool_results).await?;
        log_debug!(
            "üõ†Ô∏è Iris: Synthesis prompt built - {} chars",
            synthesis_prompt.len()
        );

        let intelligence_result = self.analyze_with_backend(&synthesis_prompt).await?;
        log_debug!(
            "ü§ñ Iris: LLM synthesis response received - {} chars",
            intelligence_result.len()
        );

        // Step 4: Parse the synthesized analysis
        log_debug!("üîç Iris: Parsing synthesized analysis result");
        iris_status_analysis!();

        // For now, fall back to basic git context to maintain compatibility
        // TODO: Build IntelligentContext entirely from tool synthesis
        let git_context = context.git_repo.get_git_info(&context.config).await?;
        let intelligent_context = self
            .parse_intelligence_result(&intelligence_result, &git_context)
            .await?;

        log_debug!(
            "‚úÖ Iris: Intelligent context gathered via agent-driven tools - {} files analyzed",
            intelligent_context.files_with_relevance.len()
        );

        Ok(intelligent_context)
    }

    /// Let the agent plan which tools to use for context analysis
    async fn plan_tool_usage_for_context_analysis(
        &self,
        _context: &AgentContext,
    ) -> Result<Vec<ToolPlan>> {
        log_debug!("ü§ñ Iris: Agent planning tool usage strategy");

        // Get available tools
        let available_tools: Vec<String> = self
            .tools
            .iter()
            .map(|tool| format!("{}: {}", tool.name(), tool.description()))
            .collect();

        log_debug!("üîß Iris: Available tools: {}", available_tools.len());

        // Create initial tool planning prompt
        let planning_prompt = format!(
            "You are Iris, an intelligent AI assistant specialized in Git workflow automation and analysis. Create an initial plan for tools to use to gather comprehensive context.\n\n\
            Your Task: Analyze Git changes to understand their purpose, impact, and relevance for generating a high-quality commit message.\n\n\
            Available tools at your disposal:\n{}\n\n\
            Repository context:\n\
            - This is a Git repository with staged changes\n\
            - You need to understand what changed and why\n\
            - You should gather enough context to assess file relevance and change impact\n\
            - You can expand or adjust this plan as you discover more context\n\n\
            As Iris, respond with a JSON array of tool calls in the order you want to execute them:\n\
            [\n\
              {{\n\
                \"tool_name\": \"Git Operations\",\n\
                \"operation\": \"diff\",\n\
                \"parameters\": {{}},\n\
                \"reason\": \"I need to see the actual code changes to understand what was modified\",\n\
                \"priority\": \"high\"\n\
              }}\n\
            ]\n\n\
            Start with 2-3 essential tool calls. You can expand your plan based on what you discover.",
            available_tools.join("\n")
        );

        log_debug!("ü§ñ Iris: Sending tool planning request to LLM");
        let planning_result = self.analyze_with_backend(&planning_prompt).await?;
        log_debug!(
            "üìã Iris: Tool planning response received - {} chars",
            planning_result.len()
        );

        // Parse the tool plan
        let tool_plan = self.parse_tool_plan(&planning_result)?;
        log_debug!(
            "üìã Iris: Parsed {} planned tool operations",
            tool_plan.len()
        );

        Ok(tool_plan)
    }

    /// Execute the planned tool calls with dynamic plan adjustment
    async fn execute_planned_tool_calls(
        &self,
        context: &AgentContext,
        initial_plan: &[ToolPlan],
    ) -> Result<Vec<ToolResult>> {
        let mut results = Vec::new();
        let mut current_plan = initial_plan.to_vec();
        let mut executed_tools = std::collections::HashSet::new();

        log_debug!(
            "üîß Iris: Starting adaptive tool execution with {} initial tools",
            current_plan.len()
        );

        while !current_plan.is_empty() {
            // Execute the next tool in the plan
            let plan = current_plan.remove(0);
            let plan_key = format!(
                "{}:{}",
                plan.tool_name,
                plan.operation.as_deref().unwrap_or("default")
            );

            // Skip if we've already executed this exact tool+operation combination
            if executed_tools.contains(&plan_key) {
                log_debug!("‚è≠Ô∏è Iris: Skipping already executed tool: {}", plan_key);
                continue;
            }

            log_debug!(
                "üîß Iris: Executing tool: {} ({})",
                plan.tool_name,
                plan.reason
            );
            iris_status_tool!(&plan.tool_name, &plan.reason);

            // Find the tool by name
            let tool = self
                .tools
                .iter()
                .find(|t| t.name() == plan.tool_name)
                .ok_or_else(|| anyhow::anyhow!("Tool not found: {}", plan.tool_name))?;

            // Execute the tool with planned parameters
            let mut params = plan.parameters.clone();
            if let Some(operation) = &plan.operation {
                params.insert(
                    "operation".to_string(),
                    serde_json::Value::String(operation.clone()),
                );
            }

            match tool.execute(context, &params).await {
                Ok(result) => {
                    log_debug!("‚úÖ Iris: Tool call completed: {}", plan.tool_name);
                    executed_tools.insert(plan_key);

                    let tool_result = ToolResult {
                        tool_name: plan.tool_name.clone(),
                        operation: plan.operation.clone(),
                        result,
                        reason: plan.reason.clone(),
                    };

                    results.push(tool_result.clone());

                    // After each tool execution, check if we need to expand the plan
                    if results.len() <= 2 {
                        // Only expand during early execution
                        log_debug!(
                            "ü§ñ Iris: Checking if plan needs expansion based on new context"
                        );
                        iris_status_expansion!();
                        let expanded_plan = self
                            .expand_plan_based_on_context(context, &results, &current_plan)
                            .await?;
                        if !expanded_plan.is_empty() {
                            log_debug!(
                                "üìã Iris: Plan expanded with {} additional tools",
                                expanded_plan.len()
                            );
                            current_plan.extend(expanded_plan);
                        }
                    }
                }
                Err(e) => {
                    log_debug!("‚ùå Iris: Tool call failed for {}: {}", plan.tool_name, e);
                    // Continue with other tools even if one fails
                }
            }
        }

        log_debug!(
            "üéØ Iris: Completed {} tool executions through adaptive planning",
            results.len()
        );
        Ok(results)
    }

    /// Expand the plan based on discovered context
    async fn expand_plan_based_on_context(
        &self,
        _context: &AgentContext,
        current_results: &[ToolResult],
        remaining_plan: &[ToolPlan],
    ) -> Result<Vec<ToolPlan>> {
        // Analyze current results to determine if we need more tools
        let mut context_summary = String::new();
        for result in current_results {
            context_summary.push_str(&format!(
                "Tool '{}' revealed: {}\n",
                result.tool_name,
                match &result.result {
                    serde_json::Value::Object(obj) => {
                        obj.get("content").and_then(|v| v.as_str()).map_or_else(
                            || "Complex data structure".to_string(),
                            |s| s.chars().take(200).collect::<String>(),
                        )
                    }
                    _ => "Non-object result".to_string(),
                }
            ));
        }

        // Get available tools not in current plan
        let planned_tools: std::collections::HashSet<String> =
            remaining_plan.iter().map(|p| p.tool_name.clone()).collect();
        let available_tools: Vec<String> = self
            .tools
            .iter()
            .filter(|tool| !planned_tools.contains(tool.name()))
            .map(|tool| format!("{}: {}", tool.name(), tool.description()))
            .collect();

        if available_tools.is_empty() {
            log_debug!("ü§ñ Iris: No additional tools available for plan expansion");
            return Ok(Vec::new());
        }

        let expansion_prompt = format!(
            "You are Iris, analyzing the context you've discovered so far. Based on what you've learned, \
            determine if you need additional tools to get a complete understanding.\n\n\
            Context you've discovered so far:\n{}\n\n\
            Your remaining planned tools:\n{}\n\n\
            Additional tools available to you:\n{}\n\n\
            As Iris, should you add more tools to your plan? If yes, respond with a JSON array of additional tool calls. \
            If you have enough context, respond with an empty array [].\n\n\
            Focus on tools that will provide missing context or deeper analysis for your understanding.",
            context_summary,
            remaining_plan
                .iter()
                .map(|p| format!("{} ({})", p.tool_name, p.reason))
                .collect::<Vec<_>>()
                .join(", "),
            available_tools.join("\n")
        );

        log_debug!("ü§ñ Iris: Requesting plan expansion from LLM");
        let expansion_result = self.analyze_with_backend(&expansion_prompt).await?;
        log_debug!(
            "üìã Iris: Plan expansion response received - {} chars",
            expansion_result.len()
        );

        let expanded_tools = self.parse_tool_plan(&expansion_result)?;
        log_debug!(
            "üìã Iris: Parsed {} additional tools for plan expansion",
            expanded_tools.len()
        );

        Ok(expanded_tools)
    }

    /// Build prompt to synthesize tool results into intelligent context
    async fn build_tool_synthesis_prompt(&self, tool_results: &[ToolResult]) -> Result<String> {
        let mut prompt = String::from(
            "You are Iris, an expert AI assistant synthesizing information from multiple tools to understand Git changes.\n\n\
            Your task is to analyze the tool results and provide intelligent insights about file relevance, change purpose, and overall impact.\n\n\
            Tool Results:\n\n",
        );

        for (i, result) in tool_results.iter().enumerate() {
            prompt.push_str(&format!(
                "=== TOOL RESULT {} ===\n\
                Tool: {}\n\
                Operation: {}\n\
                Purpose: {}\n\
                Result:\n{}\n\n",
                i + 1,
                result.tool_name,
                result.operation.as_deref().unwrap_or("N/A"),
                result.reason,
                serde_json::to_string_pretty(&result.result)
                    .unwrap_or_else(|_| "Unable to serialize".to_string())
            ));
        }

        prompt.push_str(
            "Based on these tool results, as Iris provide your analysis in this JSON format:\n\
            {\n\
              \"files\": [\n\
                {\n\
                  \"path\": \"file_path\",\n\
                  \"relevance_score\": 0.85,\n\
                  \"analysis\": \"What changed and why it matters\",\n\
                  \"key_changes\": [\"change 1\", \"change 2\"],\n\
                  \"impact_assessment\": \"How this affects the system\"\n\
                }\n\
              ],\n\
              \"change_summary\": \"Overall purpose of these changes\",\n\
              \"technical_analysis\": \"Implementation details and patterns\",\n\
              \"project_insights\": \"How this fits into the larger codebase\"\n\
            }",
        );

        Ok(prompt)
    }

    /// Parse tool planning response into structured tool plan
    fn parse_tool_plan(&self, planning_result: &str) -> Result<Vec<ToolPlan>> {
        log_debug!("üìã Iris: Parsing tool plan from LLM response");

        // Try to parse JSON response
        if let Ok(parsed) = serde_json::from_str::<serde_json::Value>(planning_result) {
            if let Some(array) = parsed.as_array() {
                let mut tool_plan = Vec::new();

                for item in array {
                    if let (Some(tool_name), Some(reason)) = (
                        item.get("tool_name").and_then(|v| v.as_str()),
                        item.get("reason").and_then(|v| v.as_str()),
                    ) {
                        let operation = item
                            .get("operation")
                            .and_then(|v| v.as_str())
                            .map(std::string::ToString::to_string);
                        let parameters = item
                            .get("parameters")
                            .and_then(|v| v.as_object())
                            .map(|obj| obj.iter().map(|(k, v)| (k.clone(), v.clone())).collect())
                            .unwrap_or_default();

                        tool_plan.push(ToolPlan {
                            tool_name: tool_name.to_string(),
                            operation,
                            parameters,
                            reason: reason.to_string(),
                        });
                    }
                }

                log_debug!(
                    "‚úÖ Iris: Successfully parsed {} tool operations",
                    tool_plan.len()
                );
                return Ok(tool_plan);
            }
        }

        // Fallback: create basic tool plan if parsing fails
        log_debug!("‚ö†Ô∏è Iris: Failed to parse tool plan, using fallback");
        Ok(vec![ToolPlan {
            tool_name: "Git Operations".to_string(),
            operation: Some("diff".to_string()),
            parameters: std::collections::HashMap::new(),
            reason: "Get code changes for analysis".to_string(),
        }])
    }

    /// Extract meaningful search terms from code changes for deeper analysis
    fn extract_search_terms_from_changes(
        &self,
        git_context: &crate::context::CommitContext,
    ) -> Vec<String> {
        let mut search_terms = Vec::new();

        for file in &git_context.staged_files {
            // Extract function names, class names, and important identifiers from diffs
            let lines: Vec<&str> = file.diff.lines().collect();
            for line in lines {
                if line.starts_with('+') || line.starts_with('-') {
                    // Look for function definitions, struct/class declarations
                    if line.contains("fn ") || line.contains("struct ") || line.contains("impl ") {
                        if let Some(term) = self.extract_identifier_from_line(line) {
                            if !search_terms.contains(&term) {
                                search_terms.push(term);
                            }
                        }
                    }
                }
            }
        }

        // Limit search terms to avoid too many tool calls
        search_terms.truncate(5);
        log_debug!(
            "üîç Iris: Extracted {} search terms: {:?}",
            search_terms.len(),
            search_terms
        );

        search_terms
    }

    /// Extract identifier from a code line
    fn extract_identifier_from_line(&self, line: &str) -> Option<String> {
        // Simple extraction logic - can be enhanced
        if line.contains("fn ") {
            if let Some(start) = line.find("fn ") {
                let after_fn = &line[start + 3..];
                if let Some(end) = after_fn.find('(') {
                    let name = after_fn[..end].trim();
                    if !name.is_empty() {
                        return Some(name.to_string());
                    }
                }
            }
        }

        if line.contains("struct ") {
            if let Some(start) = line.find("struct ") {
                let after_struct = &line[start + 7..];
                if let Some(end) = after_struct.find(' ') {
                    let name = after_struct[..end].trim();
                    if !name.is_empty() {
                        return Some(name.to_string());
                    }
                }
            }
        }

        None
    }

    /// Gather Git context using available tools
    async fn gather_git_context_with_tools(
        &self,
        context: &AgentContext,
    ) -> Result<crate::context::CommitContext> {
        log_debug!(
            "üîß Iris: Gathering Git context using {} available tools",
            self.tools.len()
        );

        // Find Git tool
        let git_tool = self
            .tools
            .iter()
            .find(|tool| tool.capabilities().contains(&"git".to_string()))
            .ok_or_else(|| anyhow::anyhow!("No Git tool available"))?;

        log_debug!("üîß Iris: Found Git tool: {}", git_tool.name());

        // Execute multiple tool calls to gather comprehensive data
        let tool_results = self.execute_multiple_tool_calls(git_tool, context).await?;

        log_debug!(
            "‚úÖ Iris: All Git tool calls completed - {} results",
            tool_results.len()
        );

        // For now, still use direct git context but enhance with tool data
        // TODO: Build CommitContext entirely from tool results
        let git_context = context.git_repo.get_git_info(&context.config).await?;

        Ok(git_context)
    }

    /// Execute multiple tool calls to gather comprehensive information
    async fn execute_multiple_tool_calls(
        &self,
        git_tool: &Arc<dyn crate::agents::tools::AgentTool>,
        context: &AgentContext,
    ) -> Result<Vec<serde_json::Value>> {
        let mut results = Vec::new();

        // Call 1: Get diff data
        log_debug!("üîß Iris: Tool call 1/4 - Getting diff data");
        let mut diff_params = std::collections::HashMap::new();
        diff_params.insert(
            "operation".to_string(),
            serde_json::Value::String("diff".to_string()),
        );

        let diff_result = git_tool.execute(context, &diff_params).await?;
        log_debug!("‚úÖ Iris: Diff tool call completed");
        results.push(diff_result);

        // Call 2: Get status
        log_debug!("üîß Iris: Tool call 2/4 - Getting repository status");
        let mut status_params = std::collections::HashMap::new();
        status_params.insert(
            "operation".to_string(),
            serde_json::Value::String("status".to_string()),
        );

        let status_result = git_tool.execute(context, &status_params).await?;
        log_debug!("‚úÖ Iris: Status tool call completed");
        results.push(status_result);

        // Call 3: Get commit log
        log_debug!("üîß Iris: Tool call 3/4 - Getting commit history");
        let mut log_params = std::collections::HashMap::new();
        log_params.insert(
            "operation".to_string(),
            serde_json::Value::String("log".to_string()),
        );

        let log_result = git_tool.execute(context, &log_params).await?;
        log_debug!("‚úÖ Iris: Log tool call completed");
        results.push(log_result);

        // Call 4: Get files list
        log_debug!("üîß Iris: Tool call 4/4 - Getting changed files list");
        let mut files_params = std::collections::HashMap::new();
        files_params.insert(
            "operation".to_string(),
            serde_json::Value::String("files".to_string()),
        );

        let files_result = git_tool.execute(context, &files_params).await?;
        log_debug!("‚úÖ Iris: Files tool call completed");
        results.push(files_result);

        log_debug!(
            "üéØ Iris: All {} tool calls executed successfully",
            results.len()
        );
        Ok(results)
    }

    /// Use file analyzer tool for deeper code analysis
    async fn analyze_files_with_tools(
        &self,
        context: &AgentContext,
        files: &[String],
    ) -> Result<Vec<serde_json::Value>> {
        log_debug!(
            "üîç Iris: Analyzing {} files with file analyzer tools",
            files.len()
        );

        // Find file analyzer tool
        let analyzer_tool = self
            .tools
            .iter()
            .find(|tool| tool.capabilities().contains(&"file_analysis".to_string()));

        if let Some(tool) = analyzer_tool {
            log_debug!("üîç Iris: Found file analyzer tool: {}", tool.name());

            let mut results = Vec::new();
            for file_path in files {
                log_debug!("üîç Iris: Analyzing file: {}", file_path);

                let mut params = std::collections::HashMap::new();
                params.insert(
                    "file_path".to_string(),
                    serde_json::Value::String(file_path.clone()),
                );
                params.insert(
                    "analysis_type".to_string(),
                    serde_json::Value::String("comprehensive".to_string()),
                );

                match tool.execute(context, &params).await {
                    Ok(result) => {
                        log_debug!("‚úÖ Iris: File analysis completed for: {}", file_path);
                        results.push(result);
                    }
                    Err(e) => {
                        log_debug!("‚ö†Ô∏è Iris: File analysis failed for {}: {}", file_path, e);
                    }
                }
            }

            log_debug!(
                "üîç Iris: File analysis completed - {} results",
                results.len()
            );
            Ok(results)
        } else {
            log_debug!("‚ö†Ô∏è Iris: No file analyzer tool available");
            Ok(Vec::new())
        }
    }

    /// Use code search tool for finding related code patterns
    async fn search_codebase_with_tools(
        &self,
        context: &AgentContext,
        search_terms: &[String],
    ) -> Result<Vec<serde_json::Value>> {
        log_debug!(
            "üîç Iris: Searching codebase for {} terms",
            search_terms.len()
        );

        // Find code search tool
        let search_tool = self.tools.iter().find(|tool| {
            tool.id().contains("search") || tool.capabilities().contains(&"search".to_string())
        });

        if let Some(tool) = search_tool {
            log_debug!("üîç Iris: Found search tool: {}", tool.name());

            let mut results = Vec::new();
            for term in search_terms {
                log_debug!("üîç Iris: Searching for: {}", term);

                let mut params = std::collections::HashMap::new();
                params.insert("query".to_string(), serde_json::Value::String(term.clone()));
                params.insert(
                    "search_type".to_string(),
                    serde_json::Value::String("comprehensive".to_string()),
                );

                match tool.execute(context, &params).await {
                    Ok(result) => {
                        log_debug!("‚úÖ Iris: Search completed for: {}", term);
                        results.push(result);
                    }
                    Err(e) => {
                        log_debug!("‚ö†Ô∏è Iris: Search failed for {}: {}", term, e);
                    }
                }
            }

            log_debug!("üîç Iris: Code search completed - {} results", results.len());
            Ok(results)
        } else {
            log_debug!("‚ö†Ô∏è Iris: No code search tool available");
            Ok(Vec::new())
        }
    }

    /// Build enhanced context analysis prompt with tool data
    async fn build_enhanced_context_analysis_prompt(
        &self,
        git_context: &CommitContext,
        file_analysis_results: &[serde_json::Value],
        search_results: &[serde_json::Value],
    ) -> Result<String> {
        log_debug!("üõ†Ô∏è Iris: Building enhanced context analysis prompt with tool data");

        let mut prompt = self.build_context_analysis_prompt(git_context).await?;

        // Add file analysis results if available
        if !file_analysis_results.is_empty() {
            prompt.push_str("\n\n=== ADDITIONAL FILE ANALYSIS ===\n");
            for (i, result) in file_analysis_results.iter().enumerate() {
                prompt.push_str(&format!(
                    "Analysis Result {}:\n{}\n\n",
                    i + 1,
                    serde_json::to_string_pretty(result)
                        .unwrap_or_else(|_| "Unable to serialize".to_string())
                ));
            }
        }

        // Add search results if available
        if !search_results.is_empty() {
            prompt.push_str("\n\n=== CODEBASE SEARCH RESULTS ===\n");
            for (i, result) in search_results.iter().enumerate() {
                prompt.push_str(&format!(
                    "Search Result {}:\n{}\n\n",
                    i + 1,
                    serde_json::to_string_pretty(result)
                        .unwrap_or_else(|_| "Unable to serialize".to_string())
                ));
            }
        }

        log_debug!(
            "üõ†Ô∏è Iris: Enhanced prompt built with {} file analyses and {} search results",
            file_analysis_results.len(),
            search_results.len()
        );

        Ok(prompt)
    }

    /// Build context analysis prompt for LLM
    async fn build_context_analysis_prompt(&self, git_context: &CommitContext) -> Result<String> {
        log_debug!(
            "üõ†Ô∏è Iris: Building context analysis prompt for {} files",
            git_context.staged_files.len()
        );

        let mut prompt = String::from(
            "You are an expert software engineer analyzing Git changes for context and relevance. \
            Your task is to intelligently analyze the provided changes and score their relevance \
            to understanding the overall purpose and impact of this commit.\n\n\
            For each file, provide:\n\
            1. Relevance score (0.0-1.0) based on how important this file is to understanding the change\n\
            2. Analysis of what changed and why it matters\n\
            3. Key changes that are most significant\n\
            4. Impact assessment on the overall system\n\n\
            Also provide:\n\
            - Overall change summary (what is the main purpose)\n\
            - Technical analysis (implementation details, patterns, architecture)\n\
            - Project insights (how this fits into the larger codebase)\n\n\
            Files to analyze:\n\n",
        );

        for (index, file) in git_context.staged_files.iter().enumerate() {
            log_debug!(
                "üìÑ Iris: Adding file {} to analysis prompt: {}",
                index + 1,
                file.path
            );
            prompt.push_str(&format!(
                "=== FILE {} ===\n\
                Path: {}\n\
                Change Type: {:?}\n\
                Diff:\n{}\n\n",
                index + 1,
                file.path,
                file.change_type,
                file.diff
            ));

            if let Some(content) = &file.content {
                log_debug!(
                    "üìÑ Iris: Including full content for file: {} ({} chars)",
                    file.path,
                    content.len()
                );
                prompt.push_str(&format!(
                    "Full Content:\n{content}\n\
                    --- End of File ---\n\n"
                ));
            }
        }

        prompt.push_str(
            "\nRespond with a JSON object in this exact format:\n\
            {\n\
              \"files\": [\n\
                {\n\
                  \"path\": \"file_path\",\n\
                  \"relevance_score\": 0.85,\n\
                  \"analysis\": \"What changed and why it matters\",\n\
                  \"key_changes\": [\"change 1\", \"change 2\"],\n\
                  \"impact_assessment\": \"How this affects the system\"\n\
                }\n\
              ],\n\
              \"change_summary\": \"Overall purpose of these changes\",\n\
              \"technical_analysis\": \"Implementation details and patterns\",\n\
              \"project_insights\": \"How this fits into the larger codebase\"\n\
            }",
        );

        log_debug!(
            "üõ†Ô∏è Iris: Context analysis prompt complete - {} chars total",
            prompt.len()
        );
        Ok(prompt)
    }

    /// Parse LLM intelligence result into structured context
    async fn parse_intelligence_result(
        &self,
        result: &str,
        git_context: &CommitContext,
    ) -> Result<IntelligentContext> {
        log_debug!(
            "üîç Iris: Parsing LLM analysis result: {}",
            result.chars().take(200).collect::<String>()
        );

        // Try to parse JSON response
        log_debug!("üîç Iris: Attempting to parse JSON response");
        if let Ok(parsed) = serde_json::from_str::<serde_json::Value>(result) {
            log_debug!("‚úÖ Iris: Successfully parsed LLM analysis JSON");

            let files_with_relevance: Vec<FileRelevance> = parsed
                .get("files")
                .and_then(|f| f.as_array())
                .map(|files| {
                    log_debug!("üìä Iris: Processing {} file analyses from LLM", files.len());
                    files
                        .iter()
                        .enumerate()
                        .filter_map(|(i, file)| {
                            let file_result = Some(FileRelevance {
                                path: file.get("path")?.as_str()?.to_string(),
                                relevance_score: file.get("relevance_score")?.as_f64()? as f32,
                                analysis: file.get("analysis")?.as_str()?.to_string(),
                                key_changes: file
                                    .get("key_changes")?
                                    .as_array()?
                                    .iter()
                                    .filter_map(|v| {
                                        v.as_str().map(std::string::ToString::to_string)
                                    })
                                    .collect(),
                                impact_assessment: file
                                    .get("impact_assessment")?
                                    .as_str()?
                                    .to_string(),
                            });

                            if let Some(ref fr) = file_result {
                                log_debug!(
                                    "üìÑ Iris: File {} analysis - relevance: {:.2}, {} key changes",
                                    fr.path,
                                    fr.relevance_score,
                                    fr.key_changes.len()
                                );
                            } else {
                                log_debug!("‚ö†Ô∏è Iris: Failed to parse file analysis #{}", i + 1);
                            }

                            file_result
                        })
                        .collect()
                })
                .unwrap_or_default();

            let change_summary = parsed
                .get("change_summary")
                .and_then(|v| v.as_str())
                .unwrap_or("Changes analyzed")
                .to_string();

            let technical_analysis = parsed
                .get("technical_analysis")
                .and_then(|v| v.as_str())
                .unwrap_or("Technical implementation details")
                .to_string();

            let project_insights = parsed
                .get("project_insights")
                .and_then(|v| v.as_str())
                .unwrap_or("Project context and fit")
                .to_string();

            log_debug!(
                "‚úÖ Iris: Successfully parsed intelligent context with {} file analyses",
                files_with_relevance.len()
            );
            log_debug!(
                "üìù Iris: Change summary: {}",
                change_summary.chars().take(100).collect::<String>()
            );

            Ok(IntelligentContext {
                files_with_relevance,
                change_summary,
                technical_analysis,
                project_insights,
            })
        } else {
            // Fallback: create basic context with equal relevance
            log_debug!("‚ö†Ô∏è Iris: Failed to parse LLM analysis JSON, using fallback context");
            log_debug!(
                "‚ö†Ô∏è Iris: JSON parsing error - raw response: {}",
                result.chars().take(500).collect::<String>()
            );

            let files_with_relevance: Vec<FileRelevance> = git_context
                .staged_files
                .iter()
                .enumerate()
                .map(|(i, file)| {
                    log_debug!(
                        "üìÑ Iris: Creating fallback analysis for file {}: {}",
                        i + 1,
                        file.path
                    );
                    FileRelevance {
                        path: file.path.clone(),
                        relevance_score: 0.7, // Default relevance
                        analysis: format!("File {} was modified", file.path),
                        key_changes: vec!["Content changes detected".to_string()],
                        impact_assessment: "Part of the overall changeset".to_string(),
                    }
                })
                .collect();

            log_debug!(
                "‚ö†Ô∏è Iris: Created fallback context with {} files",
                files_with_relevance.len()
            );

            Ok(IntelligentContext {
                files_with_relevance,
                change_summary: "Multiple files changed".to_string(),
                technical_analysis: "Various technical changes applied".to_string(),
                project_insights: "Changes contribute to project evolution".to_string(),
            })
        }
    }

    /// Build commit context from intelligent analysis
    async fn build_commit_context(
        &self,
        context: &AgentContext,
        intelligent_context: &IntelligentContext,
    ) -> Result<CommitContext> {
        log_debug!("üèóÔ∏è Iris: Building commit context from intelligent analysis");

        let git_context = context.git_repo.get_git_info(&context.config).await?;
        log_debug!(
            "üèóÔ∏è Iris: Git context retrieved with {} staged files",
            git_context.staged_files.len()
        );

        // The git_context already contains the staged files we need
        // We just need to enhance them with intelligent analysis
        let mut staged_files = git_context.staged_files.clone();

        // Enhance with intelligent analysis
        let mut enhanced_count = 0;
        for staged_file in &mut staged_files {
            if let Some(relevance_info) = intelligent_context
                .files_with_relevance
                .iter()
                .find(|f| f.path == staged_file.path)
            {
                // Replace analysis with intelligent insights
                staged_file.analysis = relevance_info.key_changes.clone();
                enhanced_count += 1;
                log_debug!(
                    "üîß Iris: Enhanced file {} with {} key changes (relevance: {:.2})",
                    staged_file.path,
                    relevance_info.key_changes.len(),
                    relevance_info.relevance_score
                );
            } else {
                log_debug!(
                    "‚ö†Ô∏è Iris: No intelligent analysis found for file: {}",
                    staged_file.path
                );
            }
        }

        log_debug!(
            "üèóÔ∏è Iris: Enhanced {} of {} files with intelligent analysis",
            enhanced_count,
            staged_files.len()
        );

        // Return the enhanced git_context with intelligent analysis
        Ok(CommitContext {
            branch: git_context.branch,
            staged_files,
            recent_commits: git_context.recent_commits,
            project_metadata: git_context.project_metadata,
            user_name: git_context.user_name,
            user_email: git_context.user_email,
        })
    }

    /// Generate text using the configured backend
    async fn generate_with_backend(
        &self,
        system_prompt: &str,
        user_prompt: &str,
    ) -> Result<String> {
        use rig::completion::Prompt;

        log_debug!("üîÆ Iris: Preparing LLM request with backend");
        log_debug!("üìä System prompt: {} chars", system_prompt.len());
        log_debug!("üë§ User prompt: {} chars", user_prompt.len());

        match &self.backend {
            AgentBackend::OpenAI { client, model } => {
                log_debug!("ü§ñ Using OpenAI backend with model: {}", model);
                let agent = client
                    .agent(model)
                    .preamble(system_prompt)
                    .temperature(0.7)
                    .max_tokens(800)
                    .build();

                log_debug!("üöÄ Sending OpenAI API request...");
                let response = agent
                    .prompt(user_prompt)
                    .await
                    .map_err(|e| anyhow::anyhow!("OpenAI API error: {}", e))?;

                log_debug!("‚úÖ OpenAI response received: {} chars", response.len());
                log_debug!(
                    "üìù Response preview: {}",
                    response.chars().take(100).collect::<String>()
                );
                Ok(response.trim().to_string())
            }
            AgentBackend::Anthropic { client, model } => {
                log_debug!("üß† Using Anthropic backend with model: {}", model);
                let agent = client
                    .agent(model)
                    .preamble(system_prompt)
                    .temperature(0.7)
                    .max_tokens(800)
                    .build();

                log_debug!("üöÄ Sending Anthropic API request...");
                let response = agent
                    .prompt(user_prompt)
                    .await
                    .map_err(|e| anyhow::anyhow!("Anthropic API error: {}", e))?;

                log_debug!("‚úÖ Anthropic response received: {} chars", response.len());
                log_debug!(
                    "üìù Response preview: {}",
                    response.chars().take(100).collect::<String>()
                );
                Ok(response.trim().to_string())
            }
        }
    }

    /// Analyze context using the backend (for intelligence gathering)
    async fn analyze_with_backend(&self, prompt: &str) -> Result<String> {
        use rig::completion::Prompt;

        let system_prompt = "You are Iris, an expert AI assistant specializing in Git workflow automation and code analysis. \
                            Provide intelligent, structured analysis in the requested JSON format. \
                            You have deep understanding of software development patterns and can provide insightful analysis.";

        log_debug!("ü§ñ Iris: Preparing intelligence analysis request");
        log_debug!("üìä Analysis prompt: {} chars", prompt.len());

        match &self.backend {
            AgentBackend::OpenAI { client, model } => {
                log_debug!(
                    "ü§ñ Using OpenAI backend for intelligence analysis with model: {}",
                    model
                );
                let agent = client
                    .agent(model)
                    .preamble(system_prompt)
                    .temperature(0.3) // Lower temperature for analysis
                    .max_tokens(1500) // More tokens for detailed analysis
                    .build();

                log_debug!("üöÄ Sending OpenAI intelligence analysis request...");
                let response = agent
                    .prompt(prompt)
                    .await
                    .map_err(|e| anyhow::anyhow!("OpenAI API error: {}", e))?;

                log_debug!(
                    "‚úÖ OpenAI intelligence analysis response received: {} chars",
                    response.len()
                );
                log_debug!(
                    "üìù Analysis response preview: {}",
                    response.chars().take(200).collect::<String>()
                );
                Ok(response.trim().to_string())
            }
            AgentBackend::Anthropic { client, model } => {
                log_debug!(
                    "üß† Using Anthropic backend for intelligence analysis with model: {}",
                    model
                );
                let agent = client
                    .agent(model)
                    .preamble(system_prompt)
                    .temperature(0.3)
                    .max_tokens(1500)
                    .build();

                log_debug!("üöÄ Sending Anthropic intelligence analysis request...");
                let response = agent
                    .prompt(prompt)
                    .await
                    .map_err(|e| anyhow::anyhow!("Anthropic API error: {}", e))?;

                log_debug!(
                    "‚úÖ Anthropic intelligence analysis response received: {} chars",
                    response.len()
                );
                log_debug!(
                    "üìù Analysis response preview: {}",
                    response.chars().take(200).collect::<String>()
                );
                Ok(response.trim().to_string())
            }
        }
    }

    /// Parse JSON response with fallback handling
    async fn parse_json_response<T>(&self, response: &str) -> Result<T>
    where
        T: for<'de> Deserialize<'de>,
    {
        log_debug!("üîç Iris: Parsing JSON response - {} chars", response.len());
        log_debug!(
            "üìù Iris: Response preview: {}",
            response.chars().take(200).collect::<String>()
        );

        // First try to parse the response directly
        log_debug!("üéØ Iris: Attempting direct JSON parsing");
        if let Ok(parsed) = serde_json::from_str::<T>(response) {
            log_debug!("‚úÖ Iris: Direct JSON parsing successful");
            return Ok(parsed);
        }
        log_debug!("‚ùå Iris: Direct JSON parsing failed, trying markdown extraction");

        // Try to extract JSON from markdown code blocks
        if let Some(json_start) = response.find("```json") {
            log_debug!(
                "üîç Iris: Found markdown JSON block at position {}",
                json_start
            );
            let content_start = json_start + 7; // Skip past "```json"
            if let Some(json_end_relative) = response[content_start..].find("```") {
                let json_end = content_start + json_end_relative;
                let json_content = &response[content_start..json_end];
                log_debug!(
                    "üìÑ Iris: Extracted JSON content - {} chars",
                    json_content.trim().len()
                );
                if let Ok(parsed) = serde_json::from_str::<T>(json_content.trim()) {
                    log_debug!("‚úÖ Iris: Markdown JSON parsing successful");
                    return Ok(parsed);
                }
                log_debug!("‚ùå Iris: Markdown JSON parsing failed");
            } else {
                log_debug!("‚ùå Iris: Found ```json but no closing ```");
            }
        } else {
            log_debug!("üîç Iris: No markdown JSON blocks found");
        }

        // Try to find any JSON object in the response
        if let Some(start) = response.find('{') {
            if let Some(end) = response.rfind('}') {
                let potential_json = &response[start..=end];
                log_debug!(
                    "üîç Iris: Found potential JSON object from {} to {} - {} chars",
                    start,
                    end,
                    potential_json.len()
                );
                log_debug!(
                    "üìÑ Iris: Potential JSON preview: {}",
                    potential_json.chars().take(100).collect::<String>()
                );
                if let Ok(parsed) = serde_json::from_str::<T>(potential_json) {
                    log_debug!("‚úÖ Iris: Extracted JSON parsing successful");
                    return Ok(parsed);
                }
                log_debug!("‚ùå Iris: Extracted JSON parsing failed");
            } else {
                log_debug!("‚ùå Iris: Found opening {{ but no closing }}");
            }
        } else {
            log_debug!("‚ùå Iris: No JSON objects found in response");
        }

        // Last resort: try to handle truncated JSON by finding the last complete field
        if let Some(start) = response.find('{') {
            // Find the last complete field by looking for the last closing brace before any truncation
            let mut brace_count = 0;
            let mut last_valid_end = start;
            
            for (i, c) in response[start..].char_indices() {
                match c {
                    '{' => brace_count += 1,
                    '}' => {
                        brace_count -= 1;
                        if brace_count == 0 {
                            last_valid_end = start + i;
                            break;
                        }
                    }
                    _ => {}
                }
            }
            
            if last_valid_end > start {
                let truncated_json = &response[start..=last_valid_end];
                log_debug!(
                    "üîß Iris: Attempting to parse truncated JSON - {} chars",
                    truncated_json.len()
                );
                if let Ok(parsed) = serde_json::from_str::<T>(truncated_json) {
                    log_debug!("‚úÖ Iris: Truncated JSON parsing successful");
                    return Ok(parsed);
                }
            }
        }

        log_debug!("üö® Iris: All JSON parsing attempts failed");
        Err(anyhow::anyhow!(
            "Failed to parse JSON response. Raw response: {}",
            response.chars().take(1000).collect::<String>() // Limit error message length
        ))
    }
}

#[async_trait]
impl super::core::IrisAgent for IrisAgent {
    fn id(&self) -> &str {
        &self.id
    }

    fn name(&self) -> &str {
        &self.name
    }

    fn description(&self) -> &str {
        &self.description
    }

    fn capabilities(&self) -> Vec<String> {
        self.capabilities.clone()
    }

    async fn execute_task(
        &self,
        task: &str,
        context: &AgentContext,
        params: &HashMap<String, serde_json::Value>,
    ) -> Result<TaskResult> {
        if !self.initialized {
            log_debug!(
                "‚ùå Iris: Agent not initialized, cannot execute task: {}",
                task
            );
            return Err(anyhow::anyhow!("Iris agent not initialized"));
        }

        log_debug!("üéØ Iris: Starting task execution: {}", task);
        log_debug!("üìã Iris: Task parameters: {} keys", params.len());

        let start_time = std::time::Instant::now();

        let result = match task {
            "generate_commit_message" | "commit_message_generation" => {
                log_debug!("üìù Iris: Executing commit message generation");
                self.generate_commit_message(context, params).await
            }
            "generate_code_review" | "code_review" | "review_code" => {
                log_debug!("üîç Iris: Executing code review generation");
                self.generate_code_review(context, params).await
            }
            "generate_pull_request" | "pull_request_description" => {
                log_debug!("üìã Iris: Executing pull request description generation");
                self.generate_pull_request(context, params).await
            }
            _ => {
                log_debug!("‚ùå Iris: Unknown task requested: {}", task);
                Err(anyhow::anyhow!("Unknown task for Iris: {}", task))
            }
        };

        let duration = start_time.elapsed();

        match &result {
            Ok(task_result) => {
                log_debug!(
                    "‚úÖ Iris: Task '{}' completed successfully in {:.2}s (confidence: {:.2})",
                    task,
                    duration.as_secs_f64(),
                    task_result.confidence
                );
            }
            Err(e) => {
                log_debug!(
                    "‚ùå Iris: Task '{}' failed after {:.2}s: {}",
                    task,
                    duration.as_secs_f64(),
                    e
                );
            }
        }

        result
    }

    fn can_handle_task(&self, task: &str) -> bool {
        matches!(
            task,
            "generate_commit_message"
                | "commit_message_generation"
                | "generate_code_review"
                | "code_review"
                | "review_code"
                | "generate_pull_request"
                | "pull_request_description"
                | "changelog_generation"
                | "file_analysis"
                | "relevance_scoring"
        )
    }

    fn task_priority(&self, task: &str) -> u8 {
        match task {
            "generate_commit_message" | "commit_message_generation" => 10,
            "generate_code_review" | "code_review" | "review_code" => 10,
            "generate_pull_request" | "pull_request_description" => 10,
            "changelog_generation" => 9,
            "file_analysis" | "relevance_scoring" => 8,
            _ => 0,
        }
    }

    async fn initialize(&mut self, _context: &AgentContext) -> Result<()> {
        self.initialized = true;
        log_debug!("ü§ñ Iris: Agent initialized successfully");
        Ok(())
    }

    async fn cleanup(&self) -> Result<()> {
        log_debug!("ü§ñ Iris: Agent cleanup completed");
        Ok(())
    }
}
