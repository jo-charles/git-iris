use anyhow::Result;
use async_trait::async_trait;
use futures::StreamExt;
use rig::completion::Prompt;
use rig::prelude::*;
use rig::streaming::StreamingPrompt;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::fmt::Write;
use std::sync::Arc;

use crate::agents::{
    core::{AgentBackend, AgentContext, TaskResult},
    tools::AgentTool,
};
use crate::commit::prompt::{
    create_pr_system_prompt, create_pr_user_prompt, create_review_system_prompt,
    create_review_user_prompt, create_system_prompt, create_user_prompt,
};
use crate::commit::review::GeneratedReview;
use crate::commit::types::{GeneratedMessage, GeneratedPullRequest};
use crate::context::CommitContext;
use crate::log_debug;
use crate::{
    iris_status_analysis, iris_status_completed, iris_status_expansion, iris_status_generation,
    iris_status_planning, iris_status_synthesis, iris_status_tool,
};

/// The unified Iris agent - an AI assistant for all Git-Iris operations
/// Optimized for maximum Rig framework efficiency
pub struct IrisAgent {
    id: String,
    name: String,
    description: String,
    capabilities: Vec<String>,
    backend: AgentBackend,
    tools: Vec<Arc<dyn AgentTool>>,
    initialized: bool,
}

/// Intelligence context gathered through LLM analysis
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IntelligentContext {
    pub files_with_relevance: Vec<FileRelevance>,
    pub change_summary: String,
    pub technical_analysis: String,
    pub project_insights: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FileRelevance {
    pub path: String,
    pub relevance_score: f32,
    pub analysis: String,
    pub key_changes: Vec<String>,
    pub impact_assessment: String,
}

/// Plan for tool execution generated by the agent
#[derive(Debug, Clone)]
pub struct ToolPlan {
    pub tool_name: String,
    pub operation: Option<String>,
    pub parameters: std::collections::HashMap<String, serde_json::Value>,
    pub reason: String,
}

/// Result from tool execution
#[derive(Debug, Clone)]
pub struct ToolResult {
    pub tool_name: String,
    pub operation: Option<String>,
    pub result: serde_json::Value,
    pub reason: String,
}

/// Streaming callback trait for real-time feedback
#[async_trait]
pub trait StreamingCallback: Send + Sync {
    /// Called when a new chunk of text is received
    async fn on_chunk(&self, chunk: &str) -> Result<()>;

    /// Called when streaming is complete
    async fn on_complete(&self, full_response: &str) -> Result<()>;

    /// Called when an error occurs during streaming
    async fn on_error(&self, error: &anyhow::Error) -> Result<()>;
}

/// Default streaming callback that updates the UI spinner
pub struct IrisStreamingCallback {
    show_chunks: bool,
}

impl IrisStreamingCallback {
    pub fn new(show_chunks: bool) -> Self {
        Self { show_chunks }
    }
}

#[async_trait]
impl StreamingCallback for IrisStreamingCallback {
    async fn on_chunk(&self, chunk: &str) -> Result<()> {
        if self.show_chunks && !chunk.trim().is_empty() {
            // Update status with streaming indicator
            crate::agents::status::IRIS_STATUS
                .update(crate::agents::status::IrisStatus::generation());
        }
        Ok(())
    }

    async fn on_complete(&self, _full_response: &str) -> Result<()> {
        crate::agents::status::IRIS_STATUS.update(crate::agents::status::IrisStatus::completed());
        Ok(())
    }

    async fn on_error(&self, error: &anyhow::Error) -> Result<()> {
        crate::agents::status::IRIS_STATUS.update(crate::agents::status::IrisStatus::error(
            &format!("Stream error: {error}"),
        ));
        Ok(())
    }
}

/// Iris's knowledge base - notes taken during task execution
#[derive(Debug, Clone, Serialize, Deserialize, Default)]
pub struct IrisKnowledge {
    pub project_insights: Vec<String>,
    pub code_patterns: Vec<String>,
    pub discovered_issues: Vec<String>,
    pub architectural_notes: Vec<String>,
    pub performance_observations: Vec<String>,
    pub security_findings: Vec<String>,
    pub learned_context: Vec<String>,
    pub tool_effectiveness: HashMap<String, String>,
    pub execution_metadata: HashMap<String, serde_json::Value>,
}

impl IrisKnowledge {
    /// Add a new insight to Iris's knowledge base
    pub fn add_insight(&mut self, category: &str, insight: String) {
        match category {
            "project" => self.project_insights.push(insight),
            "patterns" => self.code_patterns.push(insight),
            "issues" => self.discovered_issues.push(insight),
            "architecture" => self.architectural_notes.push(insight),
            "performance" => self.performance_observations.push(insight),
            "security" => self.security_findings.push(insight),
            "context" => self.learned_context.push(insight),
            _ => self.learned_context.push(format!("[{category}] {insight}")),
        }
    }

    /// Get a summary of current knowledge for context
    pub fn get_summary(&self) -> String {
        let mut summary = String::new();

        if !self.project_insights.is_empty() {
            summary.push_str("**Project Insights:**\n");
            for insight in &self.project_insights {
                writeln!(summary, "- {insight}").unwrap();
            }
            summary.push('\n');
        }

        if !self.code_patterns.is_empty() {
            summary.push_str("**Code Patterns Observed:**\n");
            for pattern in &self.code_patterns {
                writeln!(summary, "- {pattern}").unwrap();
            }
            summary.push('\n');
        }

        if !self.discovered_issues.is_empty() {
            summary.push_str("**Issues Discovered:**\n");
            for issue in &self.discovered_issues {
                writeln!(summary, "- {issue}").unwrap();
            }
            summary.push('\n');
        }

        if !self.security_findings.is_empty() {
            summary.push_str("**Security Findings:**\n");
            for finding in &self.security_findings {
                writeln!(summary, "- {finding}").unwrap();
            }
            summary.push('\n');
        }

        summary
    }
}

impl IrisAgent {
    pub fn new(backend: AgentBackend, tools: Vec<Arc<dyn AgentTool>>) -> Self {
        let mut agent = Self {
            id: "iris_agent".to_string(),
            name: "Iris".to_string(),
            description: "AI assistant for intelligent Git workflow automation and analysis"
                .to_string(),
            capabilities: vec![
                "commit_message_generation".to_string(),
                "code_review".to_string(),
                "pull_request_description".to_string(),
                "changelog_generation".to_string(),
                "file_analysis".to_string(),
                "relevance_scoring".to_string(),
                "intelligent_context_gathering".to_string(),
                "diff_analysis".to_string(),
                "change_summarization".to_string(),
                "commit_validation".to_string(),
                "security_analysis".to_string(),
                "performance_analysis".to_string(),
                "documentation_review".to_string(),
                "context_summarization".to_string(),
                "chunked_analysis".to_string(),
                "knowledge_building".to_string(),
                "adaptive_learning".to_string(),
            ],
            backend,
            tools,
            initialized: false,
        };

        agent.initialized = true;
        agent
    }

    /// Use LLM to intelligently manage context for code reviews
    async fn manage_review_context(&self, context: &CommitContext) -> Result<String> {
        iris_status_analysis!();

        log_debug!("üß† Iris: Using LLM to intelligently manage review context");

        let full_context = create_review_user_prompt(context);
        let context_size = full_context.len();

        log_debug!("üìè Iris: Full context size: {} characters", context_size);

        // If context is reasonable size, use it directly
        if context_size < 8000 {
            log_debug!("‚úÖ Iris: Context size manageable, proceeding with full review");
            return Ok(full_context);
        }

        // Let LLM intelligently summarize and prioritize
        let smart_context_prompt = format!(
            "You are Iris, an expert code reviewer. The code context below is too large for optimal review. 
            
            Your task: Create a focused, intelligent summary that preserves all critical information needed for a comprehensive code review.
            
            **What to preserve:**
            - All security-critical changes
            - Complex logic that needs careful review  
            - Performance-sensitive code
            - Error handling patterns
            - API changes or breaking changes
            - Critical diff sections (keep exact code)
            
            **What to summarize:**
            - Repetitive patterns
            - Simple refactoring
            - Formatting changes
            - Non-critical utility functions
            
            **Original Context ({context_size} chars):**
            {full_context}
            
            Create an intelligent, focused review context that captures everything important while being concise enough for thorough analysis."
        );

        let managed_context = self.analyze_with_backend(&smart_context_prompt).await?;
        log_debug!(
            "‚úÖ Iris: Created LLM-managed context - {} chars (reduced from {})",
            managed_context.len(),
            context_size
        );

        Ok(managed_context)
    }

    /// Generate a commit message with intelligent context analysis
    pub async fn generate_commit_message(
        &self,
        context: &AgentContext,
        params: &HashMap<String, serde_json::Value>,
    ) -> Result<TaskResult> {
        let preset = params
            .get("preset")
            .and_then(|v| v.as_str())
            .unwrap_or("default");
        let _custom_instructions = params
            .get("instructions")
            .and_then(|v| v.as_str())
            .unwrap_or("");
        let use_gitmoji = params
            .get("gitmoji")
            .and_then(serde_json::Value::as_bool)
            .unwrap_or(context.config.use_gitmoji);

        log_debug!(
            "ü§ñ Iris: Generating commit message with preset: '{}', gitmoji: {}",
            preset,
            use_gitmoji
        );

        // Step 1: Gather intelligent context using LLM analysis
        log_debug!("üß† Iris: Gathering intelligent context through LLM analysis");
        let intelligent_context = self.gather_intelligent_context(context).await?;

        // Step 2: Build Git context from intelligent analysis
        let commit_context = self
            .build_commit_context(context, &intelligent_context)
            .await?;

        // Step 3: Generate commit message using existing prompt system
        log_debug!("üìù Iris: Building system and user prompts");
        let system_prompt = create_system_prompt(&context.config)?;
        let user_prompt = create_user_prompt(&commit_context);

        log_debug!(
            "üìè Iris: Prompts built - System: {} chars, User: {} chars",
            system_prompt.len(),
            user_prompt.len()
        );

        // Step 4: Generate with LLM
        log_debug!("ü§ñ Iris: Generating commit message with LLM");
        iris_status_generation!();
        let generated_message = self
            .generate_with_backend(&system_prompt, &user_prompt)
            .await?;

        // Step 5: Parse and validate response
        let parsed_response = self.parse_json_response::<GeneratedMessage>(&generated_message)?;

        log_debug!(
            "‚úÖ Iris: Commit message generated - Title: '{}', {} chars total",
            parsed_response.title,
            parsed_response.message.len()
        );

        iris_status_completed!();
        Ok(TaskResult::success_with_data(
            "Commit message generated successfully".to_string(),
            serde_json::to_value(parsed_response)?,
        )
        .with_confidence(0.92))
    }

    /// Generate a commit message with streaming support for real-time feedback
    pub async fn generate_commit_message_streaming(
        &self,
        context: &AgentContext,
        params: &HashMap<String, serde_json::Value>,
        callback: &dyn StreamingCallback,
    ) -> Result<TaskResult> {
        let preset = params
            .get("preset")
            .and_then(|v| v.as_str())
            .unwrap_or("default");
        let _custom_instructions = params
            .get("instructions")
            .and_then(|v| v.as_str())
            .unwrap_or("");
        let use_gitmoji = params
            .get("gitmoji")
            .and_then(serde_json::Value::as_bool)
            .unwrap_or(context.config.use_gitmoji);

        log_debug!(
            "üåä Iris: Generating commit message with streaming - preset: '{}', gitmoji: {}",
            preset,
            use_gitmoji
        );

        // Step 1: Gather intelligent context using LLM analysis
        crate::agents::status::IRIS_STATUS.update(crate::agents::status::IrisStatus::analysis());
        let intelligent_context = self.gather_intelligent_context(context).await?;

        // Step 2: Build Git context from intelligent analysis
        crate::agents::status::IRIS_STATUS.update(crate::agents::status::IrisStatus::synthesis());
        let commit_context = self
            .build_commit_context(context, &intelligent_context)
            .await?;

        // Step 3: Create prompts
        crate::agents::status::IRIS_STATUS.update(crate::agents::status::IrisStatus::planning());
        let mut config_clone = (*context.config).clone();
        config_clone.use_gitmoji = use_gitmoji;

        let system_prompt = create_system_prompt(&config_clone)?;
        let user_prompt = create_user_prompt(&commit_context);

        // Step 4: Generate with LLM using streaming
        crate::agents::status::IRIS_STATUS.update(crate::agents::status::IrisStatus::generation());
        let generated_message = self
            .generate_with_backend_streaming(&system_prompt, &user_prompt, callback)
            .await?;

        // Step 5: Parse and validate response
        crate::agents::status::IRIS_STATUS.update(crate::agents::status::IrisStatus::synthesis());
        let parsed_response = self.parse_json_response::<GeneratedMessage>(&generated_message)?;

        log_debug!(
            "‚úÖ Iris: Streaming commit message generated - Title: '{}', {} chars total",
            parsed_response.title,
            parsed_response.message.len()
        );

        iris_status_completed!();
        Ok(TaskResult::success_with_data(
            "Commit message generated successfully with streaming".to_string(),
            serde_json::to_value(parsed_response)?,
        )
        .with_confidence(0.92))
    }

    /// Generate a code review with intelligent analysis
    pub async fn generate_code_review(
        &self,
        context: &AgentContext,
        _params: &HashMap<String, serde_json::Value>,
    ) -> Result<TaskResult> {
        log_debug!("üîç Iris: Starting intelligent code review");

        // Gather intelligent context
        let intelligent_context = self.gather_intelligent_context(context).await?;
        let commit_context = self
            .build_commit_context(context, &intelligent_context)
            .await?;

        // Generate review using existing prompt system with intelligent context management
        let system_prompt = create_review_system_prompt(&context.config)?;
        let managed_user_prompt = self.manage_review_context(&commit_context).await?;

        let generated_review = self
            .generate_with_backend(&system_prompt, &managed_user_prompt)
            .await?;
        let parsed_response = self.parse_json_response::<GeneratedReview>(&generated_review)?;

        log_debug!(
            "‚úÖ Iris: Code review completed with {} issues",
            parsed_response.issues.len()
        );

        Ok(TaskResult::success_with_data(
            "Code review generated successfully".to_string(),
            serde_json::to_value(parsed_response)?,
        )
        .with_confidence(0.88))
    }

    /// Generate a pull request description with intelligent analysis
    pub async fn generate_pull_request(
        &self,
        context: &AgentContext,
        params: &HashMap<String, serde_json::Value>,
    ) -> Result<TaskResult> {
        log_debug!("üìã Iris: Starting pull request description generation");

        // Get commit messages from params
        let commit_messages = params
            .get("commit_messages")
            .and_then(|v| v.as_array())
            .map(|arr| {
                arr.iter()
                    .filter_map(|v| v.as_str())
                    .map(std::string::ToString::to_string)
                    .collect::<Vec<_>>()
            })
            .unwrap_or_default();

        // Gather intelligent context
        let intelligent_context = self.gather_intelligent_context(context).await?;
        let commit_context = self
            .build_commit_context(context, &intelligent_context)
            .await?;

        // Generate PR description using existing prompt system
        let system_prompt = create_pr_system_prompt(&context.config)?;
        let user_prompt = create_pr_user_prompt(&commit_context, &commit_messages);

        let generated_pr = self
            .generate_with_backend(&system_prompt, &user_prompt)
            .await?;
        let parsed_response = self.parse_json_response::<GeneratedPullRequest>(&generated_pr)?;

        log_debug!(
            "‚úÖ Iris: PR description generated - Title: '{}'",
            parsed_response.title
        );

        Ok(TaskResult::success_with_data(
            "Pull request description generated successfully".to_string(),
            serde_json::to_value(parsed_response)?,
        )
        .with_confidence(0.90))
    }

    /// Gather intelligent context using LLM-driven tool selection and usage
    async fn gather_intelligent_context(
        &self,
        context: &AgentContext,
    ) -> Result<IntelligentContext> {
        log_debug!(
            "üß† Iris: Starting intelligent context analysis with agent-driven tool selection"
        );

        // Step 1: Let the agent decide which tools to use
        log_debug!("ü§ñ Iris: Planning tool usage strategy");
        iris_status_planning!();
        let tool_plan = self.plan_tool_usage_for_context_analysis(context).await?;
        log_debug!("üìã Iris: Agent planned {} tool operations", tool_plan.len());

        // Step 2: Execute the planned tool calls
        log_debug!("üîß Iris: Executing agent-planned tool calls");
        let tool_results = self.execute_planned_tool_calls(context, &tool_plan).await?;
        log_debug!("‚úÖ Iris: Completed {} tool operations", tool_results.len());

        // Step 3: Use LLM to synthesize tool results into intelligent context
        log_debug!("ü§ñ Iris: Synthesizing tool results with LLM");
        iris_status_synthesis!();
        let synthesis_prompt = self.build_tool_synthesis_prompt(&tool_results)?;
        log_debug!(
            "üõ†Ô∏è Iris: Synthesis prompt built - {} chars",
            synthesis_prompt.len()
        );

        let intelligence_result = self.analyze_with_backend(&synthesis_prompt).await?;
        log_debug!(
            "ü§ñ Iris: LLM synthesis response received - {} chars",
            intelligence_result.len()
        );

        // Step 4: Parse the synthesized analysis
        log_debug!("üîç Iris: Parsing synthesized analysis result");
        iris_status_analysis!();

        // For now, fall back to basic git context to maintain compatibility
        // TODO: Build IntelligentContext entirely from tool synthesis
        let git_context = context.git_repo.get_git_info(&context.config).await?;
        let intelligent_context =
            self.parse_intelligence_result(&intelligence_result, &git_context)?;

        log_debug!(
            "‚úÖ Iris: Intelligent context gathered via agent-driven tools - {} files analyzed",
            intelligent_context.files_with_relevance.len()
        );

        Ok(intelligent_context)
    }

    /// Let the agent plan which tools to use for context analysis
    async fn plan_tool_usage_for_context_analysis(
        &self,
        _context: &AgentContext,
    ) -> Result<Vec<ToolPlan>> {
        log_debug!("ü§ñ Iris: Agent planning tool usage strategy");

        // Get available tools
        let available_tools: Vec<String> = self
            .tools
            .iter()
            .map(|tool| format!("{}: {}", tool.name(), tool.description()))
            .collect();

        log_debug!("üîß Iris: Available tools: {}", available_tools.len());

        // Create initial tool planning prompt
        let planning_prompt = format!(
            "You are Iris, an intelligent AI assistant specialized in Git workflow automation and analysis. Create an initial plan for tools to use to gather comprehensive context.\n\n\
            Your Task: Analyze Git changes to understand their purpose, impact, and relevance for generating a high-quality commit message.\n\n\
            Available tools at your disposal:\n{}\n\n\
            Repository context:\n\
            - This is a Git repository with staged changes\n\
            - You need to understand what changed and why\n\
            - You should gather enough context to assess file relevance and change impact\n\
            - You can expand or adjust this plan as you discover more context\n\n\
            As Iris, respond with a JSON array of tool calls in the order you want to execute them:\n\
            [\n\
              {{\n\
                \"tool_name\": \"Git Operations\",\n\
                \"operation\": \"diff\",\n\
                \"parameters\": {{}},\n\
                \"reason\": \"I need to see the actual code changes to understand what was modified\",\n\
                \"priority\": \"high\"\n\
              }}\n\
            ]\n\n\
            Start with 2-3 essential tool calls. You can expand your plan based on what you discover.",
            available_tools.join("\n")
        );

        log_debug!("ü§ñ Iris: Sending tool planning request to LLM");
        let planning_result = self.analyze_with_backend(&planning_prompt).await?;
        log_debug!(
            "üìã Iris: Tool planning response received - {} chars",
            planning_result.len()
        );

        // Parse the tool plan
        let tool_plan = self.parse_tool_plan(&planning_result)?;
        log_debug!(
            "üìã Iris: Parsed {} planned tool operations",
            tool_plan.len()
        );

        Ok(tool_plan)
    }

    /// Execute the planned tool calls with dynamic plan adjustment
    async fn execute_planned_tool_calls(
        &self,
        context: &AgentContext,
        initial_plan: &[ToolPlan],
    ) -> Result<Vec<ToolResult>> {
        let mut results = Vec::new();
        let mut current_plan = initial_plan.to_vec();
        let mut executed_tools = std::collections::HashSet::new();

        log_debug!(
            "üîß Iris: Starting adaptive tool execution with {} initial tools",
            current_plan.len()
        );

        while !current_plan.is_empty() {
            // Execute the next tool in the plan
            let plan = current_plan.remove(0);
            let plan_key = format!(
                "{}:{}",
                plan.tool_name,
                plan.operation.as_deref().unwrap_or("default")
            );

            // Skip if we've already executed this exact tool+operation combination
            if executed_tools.contains(&plan_key) {
                log_debug!("‚è≠Ô∏è Iris: Skipping already executed tool: {}", plan_key);
                continue;
            }

            log_debug!(
                "üîß Iris: Executing tool: {} ({})",
                plan.tool_name,
                plan.reason
            );
            iris_status_tool!(&plan.tool_name, &plan.reason);

            // Find the tool by name
            let tool = self
                .tools
                .iter()
                .find(|t| t.name() == plan.tool_name)
                .ok_or_else(|| anyhow::anyhow!("Tool not found: {}", plan.tool_name))?;

            // Execute the tool with planned parameters
            let mut params = plan.parameters.clone();
            if let Some(operation) = &plan.operation {
                params.insert(
                    "operation".to_string(),
                    serde_json::Value::String(operation.clone()),
                );
            }

            match tool.execute(context, &params).await {
                Ok(result) => {
                    log_debug!("‚úÖ Iris: Tool call completed: {}", plan.tool_name);
                    executed_tools.insert(plan_key);

                    let tool_result = ToolResult {
                        tool_name: plan.tool_name.clone(),
                        operation: plan.operation.clone(),
                        result,
                        reason: plan.reason.clone(),
                    };

                    results.push(tool_result.clone());

                    // After each tool execution, check if we need to expand the plan
                    if results.len() <= 2 {
                        // Only expand during early execution
                        log_debug!(
                            "ü§ñ Iris: Checking if plan needs expansion based on new context"
                        );
                        iris_status_expansion!();
                        let expanded_plan = self
                            .expand_plan_based_on_context(context, &results, &current_plan)
                            .await?;
                        if !expanded_plan.is_empty() {
                            log_debug!(
                                "üìã Iris: Plan expanded with {} additional tools",
                                expanded_plan.len()
                            );
                            current_plan.extend(expanded_plan);
                        }
                    }
                }
                Err(e) => {
                    log_debug!("‚ùå Iris: Tool call failed for {}: {}", plan.tool_name, e);
                    // Continue with other tools even if one fails
                }
            }
        }

        log_debug!(
            "üéØ Iris: Completed {} tool executions through adaptive planning",
            results.len()
        );
        Ok(results)
    }

    /// Expand the plan based on discovered context
    async fn expand_plan_based_on_context(
        &self,
        _context: &AgentContext,
        current_results: &[ToolResult],
        remaining_plan: &[ToolPlan],
    ) -> Result<Vec<ToolPlan>> {
        // Analyze current results to determine if we need more tools
        let mut context_summary = String::new();
        for result in current_results {
            writeln!(
                context_summary,
                "Tool '{}' revealed: {}",
                result.tool_name,
                match &result.result {
                    serde_json::Value::Object(obj) => {
                        obj.get("content").and_then(|v| v.as_str()).map_or_else(
                            || "Complex data structure".to_string(),
                            |s| s.chars().take(200).collect::<String>(),
                        )
                    }
                    _ => "Non-object result".to_string(),
                }
            )
            .unwrap();
        }

        // Get available tools not in current plan
        let planned_tools: std::collections::HashSet<String> =
            remaining_plan.iter().map(|p| p.tool_name.clone()).collect();
        let available_tools: Vec<String> = self
            .tools
            .iter()
            .filter(|tool| !planned_tools.contains(tool.name()))
            .map(|tool| format!("{}: {}", tool.name(), tool.description()))
            .collect();

        if available_tools.is_empty() {
            log_debug!("ü§ñ Iris: No additional tools available for plan expansion");
            return Ok(Vec::new());
        }

        let expansion_prompt = format!(
            "You are Iris, analyzing the context you've discovered so far. Based on what you've learned, \
            determine if you need additional tools to get a complete understanding.\n\n\
            Context you've discovered so far:\n{}\n\n\
            Your remaining planned tools:\n{}\n\n\
            Additional tools available to you:\n{}\n\n\
            As Iris, should you add more tools to your plan? If yes, respond with a JSON array of additional tool calls. \
            If you have enough context, respond with an empty array [].\n\n\
            Focus on tools that will provide missing context or deeper analysis for your understanding.",
            context_summary,
            remaining_plan
                .iter()
                .map(|p| format!("{} ({})", p.tool_name, p.reason))
                .collect::<Vec<_>>()
                .join(", "),
            available_tools.join("\n")
        );

        log_debug!("ü§ñ Iris: Requesting plan expansion from LLM");
        let expansion_result = self.analyze_with_backend(&expansion_prompt).await?;
        log_debug!(
            "üìã Iris: Plan expansion response received - {} chars",
            expansion_result.len()
        );

        let expanded_tools = self.parse_tool_plan(&expansion_result)?;
        log_debug!(
            "üìã Iris: Parsed {} additional tools for plan expansion",
            expanded_tools.len()
        );

        Ok(expanded_tools)
    }

    /// Build prompt to synthesize tool results into intelligent context
    #[allow(clippy::unused_self, clippy::unnecessary_wraps)]
    fn build_tool_synthesis_prompt(&self, tool_results: &[ToolResult]) -> Result<String> {
        let mut prompt = String::from(
            "You are Iris, an expert AI assistant synthesizing information from multiple tools to understand Git changes.\n\n\
            Your task is to analyze the tool results and provide intelligent insights about file relevance, change purpose, and overall impact.\n\n\
            Tool Results:\n\n",
        );

        for (i, result) in tool_results.iter().enumerate() {
            write!(
                prompt,
                "=== TOOL RESULT {} ===\n\
                Tool: {}\n\
                Operation: {}\n\
                Purpose: {}\n\
                Result:\n{}\n\n",
                i + 1,
                result.tool_name,
                result.operation.as_deref().unwrap_or("N/A"),
                result.reason,
                serde_json::to_string_pretty(&result.result)
                    .unwrap_or_else(|_| "Unable to serialize".to_string())
            )
            .unwrap();
        }

        prompt.push_str(
            "Based on these tool results, as Iris provide your analysis in this JSON format:\n\
            {\n\
              \"files\": [\n\
                {\n\
                  \"path\": \"file_path\",\n\
                  \"relevance_score\": 0.85,\n\
                  \"analysis\": \"What changed and why it matters\",\n\
                  \"key_changes\": [\"change 1\", \"change 2\"],\n\
                  \"impact_assessment\": \"How this affects the system\"\n\
                }\n\
              ],\n\
              \"change_summary\": \"Overall purpose of these changes\",\n\
              \"technical_analysis\": \"Implementation details and patterns\",\n\
              \"project_insights\": \"How this fits into the larger codebase\"\n\
            }",
        );

        Ok(prompt)
    }

    /// Parse tool planning response into structured tool plan
    #[allow(clippy::unused_self, clippy::unnecessary_wraps)]
    fn parse_tool_plan(&self, planning_result: &str) -> Result<Vec<ToolPlan>> {
        log_debug!("üìã Iris: Parsing tool plan from LLM response");

        // Try to parse JSON response
        if let Ok(parsed) = serde_json::from_str::<serde_json::Value>(planning_result) {
            if let Some(array) = parsed.as_array() {
                let mut tool_plan = Vec::new();

                for item in array {
                    if let (Some(tool_name), Some(reason)) = (
                        item.get("tool_name").and_then(|v| v.as_str()),
                        item.get("reason").and_then(|v| v.as_str()),
                    ) {
                        let operation = item
                            .get("operation")
                            .and_then(|v| v.as_str())
                            .map(std::string::ToString::to_string);
                        let parameters = item
                            .get("parameters")
                            .and_then(|v| v.as_object())
                            .map(|obj| obj.iter().map(|(k, v)| (k.clone(), v.clone())).collect())
                            .unwrap_or_default();

                        tool_plan.push(ToolPlan {
                            tool_name: tool_name.to_string(),
                            operation,
                            parameters,
                            reason: reason.to_string(),
                        });
                    }
                }

                log_debug!(
                    "‚úÖ Iris: Successfully parsed {} tool operations",
                    tool_plan.len()
                );
                return Ok(tool_plan);
            }
        }

        // Fallback: create basic tool plan if parsing fails
        log_debug!("‚ö†Ô∏è Iris: Failed to parse tool plan, using fallback");
        Ok(vec![ToolPlan {
            tool_name: "Git Operations".to_string(),
            operation: Some("diff".to_string()),
            parameters: std::collections::HashMap::new(),
            reason: "Get code changes for analysis".to_string(),
        }])
    }

    /// Parse LLM intelligence result into structured context
    #[allow(clippy::unused_self, clippy::unnecessary_wraps, clippy::too_many_lines)]
    fn parse_intelligence_result(
        &self,
        result: &str,
        git_context: &CommitContext,
    ) -> Result<IntelligentContext> {
        log_debug!(
            "üîç Iris: Parsing LLM analysis result: {}",
            result.chars().take(200).collect::<String>()
        );

        // Try to parse JSON response
        log_debug!("üîç Iris: Attempting to parse JSON response");
        if let Ok(parsed) = serde_json::from_str::<serde_json::Value>(result) {
            log_debug!("‚úÖ Iris: Successfully parsed LLM analysis JSON");

            let files_with_relevance: Vec<FileRelevance> = parsed
                .get("files")
                .and_then(|f| f.as_array())
                .map(|files| {
                    log_debug!("üìä Iris: Processing {} file analyses from LLM", files.len());
                    files
                        .iter()
                        .enumerate()
                        .filter_map(|(i, file)| {
                            let file_result = Some(FileRelevance {
                                path: file.get("path")?.as_str()?.to_string(),
                                #[allow(clippy::cast_possible_truncation, clippy::as_conversions)]
                                relevance_score: file.get("relevance_score")?.as_f64()? as f32,
                                analysis: file.get("analysis")?.as_str()?.to_string(),
                                key_changes: file
                                    .get("key_changes")?
                                    .as_array()?
                                    .iter()
                                    .filter_map(|v| {
                                        v.as_str().map(std::string::ToString::to_string)
                                    })
                                    .collect(),
                                impact_assessment: file
                                    .get("impact_assessment")?
                                    .as_str()?
                                    .to_string(),
                            });

                            if let Some(ref fr) = file_result {
                                log_debug!(
                                    "üìÑ Iris: File {} analysis - relevance: {:.2}, {} key changes",
                                    fr.path,
                                    fr.relevance_score,
                                    fr.key_changes.len()
                                );
                            } else {
                                log_debug!("‚ö†Ô∏è Iris: Failed to parse file analysis #{}", i + 1);
                            }

                            file_result
                        })
                        .collect()
                })
                .unwrap_or_default();

            let change_summary = parsed
                .get("change_summary")
                .and_then(|v| v.as_str())
                .unwrap_or("Changes analyzed")
                .to_string();

            let technical_analysis = parsed
                .get("technical_analysis")
                .and_then(|v| v.as_str())
                .unwrap_or("Technical implementation details")
                .to_string();

            let project_insights = parsed
                .get("project_insights")
                .and_then(|v| v.as_str())
                .unwrap_or("Project context and fit")
                .to_string();

            log_debug!(
                "‚úÖ Iris: Successfully parsed intelligent context with {} file analyses",
                files_with_relevance.len()
            );
            log_debug!(
                "üìù Iris: Change summary: {}",
                change_summary.chars().take(100).collect::<String>()
            );

            Ok(IntelligentContext {
                files_with_relevance,
                change_summary,
                technical_analysis,
                project_insights,
            })
        } else {
            // Fallback: create basic context with equal relevance
            log_debug!("‚ö†Ô∏è Iris: Failed to parse LLM analysis JSON, using fallback context");
            log_debug!(
                "‚ö†Ô∏è Iris: JSON parsing error - raw response: {}",
                result.chars().take(500).collect::<String>()
            );

            let files_with_relevance: Vec<FileRelevance> = git_context
                .staged_files
                .iter()
                .enumerate()
                .map(|(i, file)| {
                    log_debug!(
                        "üìÑ Iris: Creating fallback analysis for file {}: {}",
                        i + 1,
                        file.path
                    );
                    FileRelevance {
                        path: file.path.clone(),
                        relevance_score: 0.7, // Default relevance
                        analysis: format!("File {} was modified", file.path),
                        key_changes: vec!["Content changes detected".to_string()],
                        impact_assessment: "Part of the overall changeset".to_string(),
                    }
                })
                .collect();

            log_debug!(
                "‚ö†Ô∏è Iris: Created fallback context with {} files",
                files_with_relevance.len()
            );

            Ok(IntelligentContext {
                files_with_relevance,
                change_summary: "Multiple files changed".to_string(),
                technical_analysis: "Various technical changes applied".to_string(),
                project_insights: "Changes contribute to project evolution".to_string(),
            })
        }
    }

    /// Build commit context from intelligent analysis
    async fn build_commit_context(
        &self,
        context: &AgentContext,
        intelligent_context: &IntelligentContext,
    ) -> Result<CommitContext> {
        log_debug!("üèóÔ∏è Iris: Building commit context from intelligent analysis");

        let git_context = context.git_repo.get_git_info(&context.config).await?;
        log_debug!(
            "üèóÔ∏è Iris: Git context retrieved with {} staged files",
            git_context.staged_files.len()
        );

        // The git_context already contains the staged files we need
        // We just need to enhance them with intelligent analysis
        let mut staged_files = git_context.staged_files.clone();

        // Enhance with intelligent analysis
        let mut enhanced_count = 0;
        for staged_file in &mut staged_files {
            if let Some(relevance_info) = intelligent_context
                .files_with_relevance
                .iter()
                .find(|f| f.path == staged_file.path)
            {
                // Replace analysis with intelligent insights
                staged_file.analysis = relevance_info.key_changes.clone();
                enhanced_count += 1;
                log_debug!(
                    "üîß Iris: Enhanced file {} with {} key changes (relevance: {:.2})",
                    staged_file.path,
                    relevance_info.key_changes.len(),
                    relevance_info.relevance_score
                );
            } else {
                log_debug!(
                    "‚ö†Ô∏è Iris: No intelligent analysis found for file: {}",
                    staged_file.path
                );
            }
        }

        log_debug!(
            "üèóÔ∏è Iris: Enhanced {} of {} files with intelligent analysis",
            enhanced_count,
            staged_files.len()
        );

        // Return the enhanced git_context with intelligent analysis
        Ok(CommitContext {
            branch: git_context.branch,
            staged_files,
            recent_commits: git_context.recent_commits,
            project_metadata: git_context.project_metadata,
            user_name: git_context.user_name,
            user_email: git_context.user_email,
        })
    }

    /// Generate text using Rig's optimized agent builder pattern
    async fn generate_with_backend(
        &self,
        system_prompt: &str,
        user_prompt: &str,
    ) -> Result<String> {
        log_debug!("üîÆ Iris: Preparing optimized LLM request with Rig");
        log_debug!("üìä System prompt: {} chars", system_prompt.len());
        log_debug!("üë§ User prompt: {} chars", user_prompt.len());

        // Use Rig's fluent builder pattern for optimal configuration
        let response_text = match &self.backend {
            AgentBackend::OpenAI { client, model } => {
                log_debug!("ü§ñ Using Rig-optimized OpenAI agent builder");

                // Leverage Rig's chainable configuration for generation tasks
                let response = client
                    .agent(model)
                    .preamble(system_prompt)
                    .temperature(0.7)
                    .max_tokens(800)
                    .build()
                    .prompt(user_prompt)
                    .await
                    .map_err(|e| anyhow::anyhow!("OpenAI API error: {}", e))?;

                log_debug!("‚úÖ OpenAI response received via Rig");
                response
            }
            AgentBackend::Anthropic { client, model } => {
                log_debug!("üß† Using Rig-optimized Anthropic agent builder");

                // Leverage Rig's unified API for different providers
                let response = client
                    .agent(model)
                    .preamble(system_prompt)
                    .temperature(0.7)
                    .max_tokens(800)
                    .build()
                    .prompt(user_prompt)
                    .await
                    .map_err(|e| anyhow::anyhow!("Anthropic API error: {}", e))?;

                log_debug!("‚úÖ Anthropic response received via Rig");
                response
            }
        };

        log_debug!("‚úÖ Response processed: {} chars", response_text.len());
        log_debug!(
            "üìù Response preview: {}",
            response_text.chars().take(100).collect::<String>()
        );

        Ok(response_text.trim().to_string())
    }

    /// Generate text using Rig's streaming interface for real-time feedback
    async fn generate_with_backend_streaming(
        &self,
        system_prompt: &str,
        user_prompt: &str,
        callback: &dyn StreamingCallback,
    ) -> Result<String> {
        log_debug!("üåä Iris: Preparing streaming LLM request with Rig");
        log_debug!("üìä System prompt: {} chars", system_prompt.len());
        log_debug!("üë§ User prompt: {} chars", user_prompt.len());

        let mut full_response = String::new();

        // Use Rig's streaming capabilities
        let result = match &self.backend {
            AgentBackend::OpenAI { client, model } => {
                log_debug!("ü§ñ Using Rig streaming for OpenAI");

                // Create streaming agent
                let agent = client
                    .agent(model)
                    .preamble(system_prompt)
                    .temperature(0.7)
                    .max_tokens(800)
                    .build();

                // Use Rig's real streaming API
                match agent.stream_prompt(user_prompt).await {
                    Ok(mut stream) => {
                        // Process stream chunks as they arrive
                        while let Some(chunk_result) = stream.next().await {
                            match chunk_result {
                                Ok(assistant_content) => {
                                    // Extract text from AssistantContent
                                    match assistant_content {
                                        rig::completion::AssistantContent::Text(text) => {
                                            if !text.text.is_empty() {
                                                callback.on_chunk(&text.text).await?;
                                                full_response.push_str(&text.text);
                                            }
                                        }
                                        rig::completion::AssistantContent::ToolCall(_) => {
                                            // Handle tool calls if needed
                                            log_debug!(
                                                "üîß Received tool call in streaming response"
                                            );
                                        }
                                    }
                                }
                                Err(e) => {
                                    let error = anyhow::anyhow!("OpenAI streaming error: {}", e);
                                    callback.on_error(&error).await?;
                                    return Err(error);
                                }
                            }
                        }

                        // The full response is accumulated in full_response
                        Ok(full_response.clone())
                    }
                    Err(e) => {
                        let error = anyhow::anyhow!("OpenAI streaming error: {}", e);
                        callback.on_error(&error).await?;
                        Err(error)
                    }
                }
            }
            AgentBackend::Anthropic { client, model } => {
                log_debug!("üß† Using Rig streaming for Anthropic");

                // Create streaming agent
                let agent = client
                    .agent(model)
                    .preamble(system_prompt)
                    .temperature(0.7)
                    .max_tokens(800)
                    .build();

                // Use Rig's real streaming API
                match agent.stream_prompt(user_prompt).await {
                    Ok(mut stream) => {
                        // Process stream chunks as they arrive
                        while let Some(chunk_result) = stream.next().await {
                            match chunk_result {
                                Ok(assistant_content) => {
                                    // Extract text from AssistantContent
                                    match assistant_content {
                                        rig::completion::AssistantContent::Text(text) => {
                                            if !text.text.is_empty() {
                                                callback.on_chunk(&text.text).await?;
                                                full_response.push_str(&text.text);
                                            }
                                        }
                                        rig::completion::AssistantContent::ToolCall(_) => {
                                            // Handle tool calls if needed
                                            log_debug!(
                                                "üîß Received tool call in streaming response"
                                            );
                                        }
                                    }
                                }
                                Err(e) => {
                                    let error = anyhow::anyhow!("Anthropic streaming error: {}", e);
                                    callback.on_error(&error).await?;
                                    return Err(error);
                                }
                            }
                        }

                        // The full response is accumulated in full_response
                        Ok(full_response.clone())
                    }
                    Err(e) => {
                        let error = anyhow::anyhow!("Anthropic streaming error: {}", e);
                        callback.on_error(&error).await?;
                        Err(error)
                    }
                }
            }
        };

        match result {
            Ok(response) => {
                callback.on_complete(&response).await?;
                log_debug!("‚úÖ Streaming response completed: {} chars", response.len());
                Ok(response.trim().to_string())
            }
            Err(e) => Err(e),
        }
    }

    /// Analyze context using Rig's optimized analysis configuration
    async fn analyze_with_backend(&self, prompt: &str) -> Result<String> {
        let system_prompt = "You are Iris, an expert AI assistant specializing in Git workflow automation and code analysis. \
                            Provide intelligent, structured analysis in the requested JSON format. \
                            You have deep understanding of software development patterns and can provide insightful analysis.";

        log_debug!("ü§ñ Iris: Preparing Rig-optimized intelligence analysis");
        log_debug!("üìä Analysis prompt: {} chars", prompt.len());

        // Use Rig's optimized configuration for analysis tasks
        let response_text = match &self.backend {
            AgentBackend::OpenAI { client, model } => {
                log_debug!("ü§ñ Using Rig-optimized OpenAI analysis configuration");

                // Configure agent specifically for analysis with Rig's builder pattern
                let response = client
                    .agent(model)
                    .preamble(system_prompt)
                    .temperature(0.3) // Lower temperature for consistent analysis
                    .max_tokens(1500) // More tokens for detailed analysis
                    .build()
                    .prompt(prompt)
                    .await
                    .map_err(|e| anyhow::anyhow!("OpenAI analysis error: {}", e))?;

                log_debug!("‚úÖ OpenAI analysis completed via Rig");
                response
            }
            AgentBackend::Anthropic { client, model } => {
                log_debug!("üß† Using Rig-optimized Anthropic analysis configuration");

                // Leverage Rig's provider abstraction for consistent interface
                let response = client
                    .agent(model)
                    .preamble(system_prompt)
                    .temperature(0.3)
                    .max_tokens(1500)
                    .build()
                    .prompt(prompt)
                    .await
                    .map_err(|e| anyhow::anyhow!("Anthropic analysis error: {}", e))?;

                log_debug!("‚úÖ Anthropic analysis completed via Rig");
                response
            }
        };

        log_debug!(
            "‚úÖ Intelligence analysis response received: {} chars",
            response_text.len()
        );
        log_debug!(
            "üìù Analysis response preview: {}",
            response_text.chars().take(200).collect::<String>()
        );

        Ok(response_text.trim().to_string())
    }

    /// Parse JSON response with fallback handling
    #[allow(clippy::unused_self, clippy::cognitive_complexity)]
    fn parse_json_response<T>(&self, response: &str) -> Result<T>
    where
        T: for<'de> Deserialize<'de>,
    {
        log_debug!("üîç Iris: Parsing JSON response - {} chars", response.len());
        log_debug!(
            "üìù Iris: Response preview: {}",
            response.chars().take(200).collect::<String>()
        );

        // First try to parse the response directly
        log_debug!("üéØ Iris: Attempting direct JSON parsing");
        if let Ok(parsed) = serde_json::from_str::<T>(response) {
            log_debug!("‚úÖ Iris: Direct JSON parsing successful");
            return Ok(parsed);
        }
        log_debug!("‚ùå Iris: Direct JSON parsing failed, trying markdown extraction");

        // Try to extract JSON from markdown code blocks
        if let Some(json_start) = response.find("```json") {
            log_debug!(
                "üîç Iris: Found markdown JSON block at position {}",
                json_start
            );
            let content_start = json_start + 7; // Skip past "```json"
            if let Some(json_end_relative) = response[content_start..].find("```") {
                let json_end = content_start + json_end_relative;
                let json_content = &response[content_start..json_end];
                log_debug!(
                    "üìÑ Iris: Extracted JSON content - {} chars",
                    json_content.trim().len()
                );
                if let Ok(parsed) = serde_json::from_str::<T>(json_content.trim()) {
                    log_debug!("‚úÖ Iris: Markdown JSON parsing successful");
                    return Ok(parsed);
                }
                log_debug!("‚ùå Iris: Markdown JSON parsing failed");
            } else {
                log_debug!("‚ùå Iris: Found ```json but no closing ```");
            }
        } else {
            log_debug!("üîç Iris: No markdown JSON blocks found");
        }

        // Try to find any JSON object in the response
        if let Some(start) = response.find('{') {
            if let Some(end) = response.rfind('}') {
                let potential_json = &response[start..=end];
                log_debug!(
                    "üîç Iris: Found potential JSON object from {} to {} - {} chars",
                    start,
                    end,
                    potential_json.len()
                );
                log_debug!(
                    "üìÑ Iris: Potential JSON preview: {}",
                    potential_json.chars().take(100).collect::<String>()
                );
                if let Ok(parsed) = serde_json::from_str::<T>(potential_json) {
                    log_debug!("‚úÖ Iris: Extracted JSON parsing successful");
                    return Ok(parsed);
                }
                log_debug!("‚ùå Iris: Extracted JSON parsing failed");
            } else {
                log_debug!("‚ùå Iris: Found opening {{ but no closing }}");
            }
        } else {
            log_debug!("‚ùå Iris: No JSON objects found in response");
        }

        // Last resort: try to handle truncated JSON by finding the last complete field
        if let Some(start) = response.find('{') {
            // Find the last complete field by looking for the last closing brace before any truncation
            let mut brace_count = 0;
            let mut last_valid_end = start;

            for (i, c) in response[start..].char_indices() {
                match c {
                    '{' => brace_count += 1,
                    '}' => {
                        brace_count -= 1;
                        if brace_count == 0 {
                            last_valid_end = start + i;
                            break;
                        }
                    }
                    _ => {}
                }
            }

            if last_valid_end > start {
                let truncated_json = &response[start..=last_valid_end];
                log_debug!(
                    "üîß Iris: Attempting to parse truncated JSON - {} chars",
                    truncated_json.len()
                );
                if let Ok(parsed) = serde_json::from_str::<T>(truncated_json) {
                    log_debug!("‚úÖ Iris: Truncated JSON parsing successful");
                    return Ok(parsed);
                }
            }
        }

        log_debug!("üö® Iris: All JSON parsing attempts failed");
        Err(anyhow::anyhow!(
            "Failed to parse JSON response. Raw response: {}",
            response.chars().take(1000).collect::<String>() // Limit error message length
        ))
    }
}

#[async_trait]
impl super::core::IrisAgent for IrisAgent {
    fn id(&self) -> &str {
        &self.id
    }

    fn name(&self) -> &str {
        &self.name
    }

    fn description(&self) -> &str {
        &self.description
    }

    fn capabilities(&self) -> Vec<String> {
        self.capabilities.clone()
    }

    async fn execute_task(
        &self,
        task: &str,
        context: &AgentContext,
        params: &HashMap<String, serde_json::Value>,
    ) -> Result<TaskResult> {
        if !self.initialized {
            log_debug!(
                "‚ùå Iris: Agent not initialized, cannot execute task: {}",
                task
            );
            return Err(anyhow::anyhow!("Iris agent not initialized"));
        }

        log_debug!("üéØ Iris: Starting task execution: {}", task);
        log_debug!("üìã Iris: Task parameters: {} keys", params.len());

        let start_time = std::time::Instant::now();

        let result = match task {
            "generate_commit_message" | "commit_message_generation" => {
                log_debug!("üìù Iris: Executing commit message generation");
                self.generate_commit_message(context, params).await
            }
            "generate_code_review" | "code_review" | "review_code" => {
                log_debug!("üîç Iris: Executing code review generation");
                self.generate_code_review(context, params).await
            }
            "generate_pull_request" | "pull_request_description" => {
                log_debug!("üìã Iris: Executing pull request description generation");
                self.generate_pull_request(context, params).await
            }
            _ => {
                log_debug!("‚ùå Iris: Unknown task requested: {}", task);
                Err(anyhow::anyhow!("Unknown task for Iris: {}", task))
            }
        };

        let duration = start_time.elapsed();

        match &result {
            Ok(task_result) => {
                log_debug!(
                    "‚úÖ Iris: Task '{}' completed successfully in {:.2}s (confidence: {:.2})",
                    task,
                    duration.as_secs_f64(),
                    task_result.confidence
                );
            }
            Err(e) => {
                log_debug!(
                    "‚ùå Iris: Task '{}' failed after {:.2}s: {}",
                    task,
                    duration.as_secs_f64(),
                    e
                );
            }
        }

        result
    }

    fn can_handle_task(&self, task: &str) -> bool {
        matches!(
            task,
            "generate_commit_message"
                | "commit_message_generation"
                | "generate_code_review"
                | "code_review"
                | "review_code"
                | "generate_pull_request"
                | "pull_request_description"
                | "changelog_generation"
                | "file_analysis"
                | "relevance_scoring"
        )
    }

    fn task_priority(&self, task: &str) -> u8 {
        match task {
            "generate_commit_message"
            | "commit_message_generation"
            | "generate_code_review"
            | "code_review"
            | "review_code"
            | "generate_pull_request"
            | "pull_request_description" => 10,
            "changelog_generation" => 9,
            "file_analysis" | "relevance_scoring" => 8,
            _ => 0,
        }
    }

    async fn initialize(&mut self, _context: &AgentContext) -> Result<()> {
        self.initialized = true;
        log_debug!("ü§ñ Iris: Agent initialized successfully");
        Ok(())
    }

    async fn cleanup(&self) -> Result<()> {
        log_debug!("ü§ñ Iris: Agent cleanup completed");
        Ok(())
    }

    fn as_any(&self) -> &dyn std::any::Any {
        self
    }
}
