use anyhow::Result;
use async_trait::async_trait;
use futures::StreamExt;
use regex::Regex;
use rig::completion::Prompt;
use rig::prelude::*;
use rig::streaming::StreamingPrompt;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::fmt::Write;
use std::sync::Arc;

use crate::agents::{
    core::{AgentBackend, AgentContext, TaskResult},
    tools::AgentTool,
};
use crate::commit::prompt::{
    create_pr_system_prompt, create_pr_user_prompt, create_review_system_prompt,
    create_review_user_prompt, create_system_prompt, create_user_prompt,
};
use crate::commit::review::GeneratedReview;
use crate::commit::types::{GeneratedMessage, GeneratedPullRequest};
use crate::context::CommitContext;
use crate::log_debug;
use crate::{
    iris_status_analysis, iris_status_completed, iris_status_expansion, iris_status_generation,
    iris_status_planning, iris_status_synthesis, iris_status_tool,
};

// Token limit for all operations - Claude can handle much more than 8192 tokens
const DEFAULT_MAX_TOKENS: u64 = 8192;

/// The unified Iris agent - an AI assistant for all Git-Iris operations
/// Optimized for maximum Rig framework efficiency
pub struct IrisAgent {
    id: String,
    name: String,
    description: String,
    capabilities: Vec<String>,
    backend: AgentBackend,
    tools: Vec<Arc<dyn AgentTool>>,
    initialized: bool,
}

/// Intelligence context gathered through LLM analysis
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IntelligentContext {
    pub files_with_relevance: Vec<FileRelevance>,
    pub change_summary: String,
    pub technical_analysis: String,
    pub project_insights: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FileRelevance {
    pub path: String,
    pub relevance_score: f32,
    pub analysis: String,
    pub key_changes: Vec<String>,
    pub impact_assessment: String,
}

/// Plan for tool execution generated by the agent
#[derive(Debug, Clone)]
pub struct ToolPlan {
    pub tool_name: String,
    pub operation: Option<String>,
    pub parameters: std::collections::HashMap<String, serde_json::Value>,
    pub reason: String,
}

/// Result from tool execution
#[derive(Debug, Clone)]
pub struct ToolResult {
    pub tool_name: String,
    pub operation: Option<String>,
    pub result: serde_json::Value,
    pub reason: String,
}

/// Streaming callback trait for real-time feedback
#[async_trait]
pub trait StreamingCallback: Send + Sync {
    /// Called when a new chunk of text is received
    async fn on_chunk(&self, chunk: &str) -> Result<()>;

    /// Called when streaming is complete
    async fn on_complete(&self, full_response: &str) -> Result<()>;

    /// Called when an error occurs during streaming
    async fn on_error(&self, error: &anyhow::Error) -> Result<()>;
}

/// Default streaming callback that updates the UI spinner
pub struct IrisStreamingCallback {
    show_chunks: bool,
}

impl IrisStreamingCallback {
    pub fn new(show_chunks: bool) -> Self {
        Self { show_chunks }
    }
}

#[async_trait]
impl StreamingCallback for IrisStreamingCallback {
    async fn on_chunk(&self, chunk: &str) -> Result<()> {
        if self.show_chunks && !chunk.trim().is_empty() {
            // Update status with streaming indicator
            crate::agents::status::IRIS_STATUS
                .update(crate::agents::status::IrisStatus::generation());
        }
        Ok(())
    }

    async fn on_complete(&self, _full_response: &str) -> Result<()> {
        crate::agents::status::IRIS_STATUS.update(crate::agents::status::IrisStatus::completed());
        Ok(())
    }

    async fn on_error(&self, error: &anyhow::Error) -> Result<()> {
        crate::agents::status::IRIS_STATUS.update(crate::agents::status::IrisStatus::error(
            &format!("Stream error: {error}"),
        ));
        Ok(())
    }
}

/// Iris's knowledge base - notes taken during task execution
#[derive(Debug, Clone, Serialize, Deserialize, Default)]
pub struct IrisKnowledge {
    pub project_insights: Vec<String>,
    pub code_patterns: Vec<String>,
    pub discovered_issues: Vec<String>,
    pub architectural_notes: Vec<String>,
    pub performance_observations: Vec<String>,
    pub security_findings: Vec<String>,
    pub learned_context: Vec<String>,
    pub tool_effectiveness: HashMap<String, String>,
    pub execution_metadata: HashMap<String, serde_json::Value>,
}

impl IrisKnowledge {
    /// Add a new insight to Iris's knowledge base
    pub fn add_insight(&mut self, category: &str, insight: String) {
        match category {
            "project" => self.project_insights.push(insight),
            "patterns" => self.code_patterns.push(insight),
            "issues" => self.discovered_issues.push(insight),
            "architecture" => self.architectural_notes.push(insight),
            "performance" => self.performance_observations.push(insight),
            "security" => self.security_findings.push(insight),
            "context" => self.learned_context.push(insight),
            _ => self.learned_context.push(format!("[{category}] {insight}")),
        }
    }

    /// Get a summary of current knowledge for context
    pub fn get_summary(&self) -> String {
        let mut summary = String::new();

        if !self.project_insights.is_empty() {
            summary.push_str("**Project Insights:**\n");
            for insight in &self.project_insights {
                writeln!(summary, "- {insight}").unwrap();
            }
            summary.push('\n');
        }

        if !self.code_patterns.is_empty() {
            summary.push_str("**Code Patterns Observed:**\n");
            for pattern in &self.code_patterns {
                writeln!(summary, "- {pattern}").unwrap();
            }
            summary.push('\n');
        }

        if !self.discovered_issues.is_empty() {
            summary.push_str("**Issues Discovered:**\n");
            for issue in &self.discovered_issues {
                writeln!(summary, "- {issue}").unwrap();
            }
            summary.push('\n');
        }

        if !self.security_findings.is_empty() {
            summary.push_str("**Security Findings:**\n");
            for finding in &self.security_findings {
                writeln!(summary, "- {finding}").unwrap();
            }
            summary.push('\n');
        }

        summary
    }
}

impl IrisAgent {
    pub fn new(backend: AgentBackend, tools: Vec<Arc<dyn AgentTool>>) -> Self {
        let mut agent = Self {
            id: "iris_agent".to_string(),
            name: "Iris".to_string(),
            description: "AI assistant for intelligent Git workflow automation and analysis"
                .to_string(),
            capabilities: vec![
                "commit_message_generation".to_string(),
                "code_review".to_string(),
                "pull_request_description".to_string(),
                "changelog_generation".to_string(),
                "release_notes_generation".to_string(),
                "file_analysis".to_string(),
                "relevance_scoring".to_string(),
                "intelligent_context_gathering".to_string(),
                "diff_analysis".to_string(),
                "change_summarization".to_string(),
                "commit_validation".to_string(),
                "security_analysis".to_string(),
                "performance_analysis".to_string(),
                "documentation_review".to_string(),
                "context_summarization".to_string(),
                "chunked_analysis".to_string(),
                "knowledge_building".to_string(),
                "adaptive_learning".to_string(),
            ],
            backend,
            tools,
            initialized: false,
        };

        agent.initialized = true;
        agent
    }

    /// Use LLM to intelligently manage context for code reviews
    async fn manage_review_context(&self, context: &CommitContext) -> Result<String> {
        iris_status_analysis!();

        log_debug!("üß† Iris: Using LLM to intelligently manage review context");

        let full_context = create_review_user_prompt(context);
        let context_size = full_context.len();

        log_debug!("üìè Iris: Full context size: {} characters", context_size);

        // If context is reasonable size, use it directly
        if context_size < 8000 {
            log_debug!("‚úÖ Iris: Context size manageable, proceeding with full review");
            return Ok(full_context);
        }

        // Let LLM intelligently summarize and prioritize
        let smart_context_prompt = format!(
            "You are Iris, an expert code reviewer. The code context below is too large for optimal review. 
            
            Your task: Create a focused, intelligent summary that preserves all critical information needed for a comprehensive code review.
            
            **What to preserve:**
            - All security-critical changes
            - Complex logic that needs careful review  
            - Performance-sensitive code
            - Error handling patterns
            - API changes or breaking changes
            - Critical diff sections (keep exact code)
            
            **What to summarize:**
            - Repetitive patterns
            - Simple refactoring
            - Formatting changes
            - Non-critical utility functions
            
            **Original Context ({context_size} chars):**
            {full_context}
            
            Create an intelligent, focused review context that captures everything important while being concise enough for thorough analysis."
        );

        let managed_context = self.analyze_with_backend(&smart_context_prompt).await?;
        log_debug!(
            "‚úÖ Iris: Created LLM-managed context - {} chars (reduced from {})",
            managed_context.len(),
            context_size
        );

        Ok(managed_context)
    }

    /// Generate a commit message with intelligent context analysis
    pub async fn generate_commit_message(
        &self,
        context: &AgentContext,
        params: &HashMap<String, serde_json::Value>,
    ) -> Result<TaskResult> {
        let preset = params
            .get("preset")
            .and_then(|v| v.as_str())
            .unwrap_or("default");
        let _custom_instructions = params
            .get("instructions")
            .and_then(|v| v.as_str())
            .unwrap_or("");
        let use_gitmoji = params
            .get("gitmoji")
            .and_then(serde_json::Value::as_bool)
            .unwrap_or(context.config.use_gitmoji);

        log_debug!(
            "ü§ñ Iris: Generating commit message with preset: '{}', gitmoji: {}",
            preset,
            use_gitmoji
        );

        // Step 1: Gather intelligent context using LLM analysis
        log_debug!("üß† Iris: Gathering intelligent context through LLM analysis");
        let intelligent_context = self.gather_intelligent_context(context).await?;

        // Step 2: Build Git context from intelligent analysis
        let commit_context = self
            .build_commit_context(context, &intelligent_context)
            .await?;

        // Step 3: Generate commit message using existing prompt system
        log_debug!("üìù Iris: Building system and user prompts");
        let system_prompt = create_system_prompt(&context.config)?;
        let user_prompt = create_user_prompt(&commit_context);

        log_debug!(
            "üìè Iris: Prompts built - System: {} chars, User: {} chars",
            system_prompt.len(),
            user_prompt.len()
        );

        // Step 4: Generate with LLM
        log_debug!("ü§ñ Iris: Generating commit message with LLM");
        iris_status_generation!();
        let generated_message = self
            .generate_with_backend(&system_prompt, &user_prompt)
            .await?;

        // Step 5: Parse and validate response
        let parsed_response = self.parse_json_response::<GeneratedMessage>(&generated_message)?;

        log_debug!(
            "‚úÖ Iris: Commit message generated - Title: '{}', {} chars total",
            parsed_response.title,
            parsed_response.message.len()
        );

        iris_status_completed!();
        Ok(TaskResult::success_with_data(
            "Commit message generated successfully".to_string(),
            serde_json::to_value(parsed_response)?,
        )
        .with_confidence(0.92))
    }

    /// Generate a commit message with streaming support for real-time feedback
    pub async fn generate_commit_message_streaming(
        &self,
        context: &AgentContext,
        params: &HashMap<String, serde_json::Value>,
        callback: &dyn StreamingCallback,
    ) -> Result<TaskResult> {
        let preset = params
            .get("preset")
            .and_then(|v| v.as_str())
            .unwrap_or("default");
        let _custom_instructions = params
            .get("instructions")
            .and_then(|v| v.as_str())
            .unwrap_or("");
        let use_gitmoji = params
            .get("gitmoji")
            .and_then(serde_json::Value::as_bool)
            .unwrap_or(context.config.use_gitmoji);

        log_debug!(
            "üåä Iris: Generating commit message with streaming - preset: '{}', gitmoji: {}",
            preset,
            use_gitmoji
        );

        // Step 1: Gather intelligent context using LLM analysis
        crate::agents::status::IRIS_STATUS.update(crate::agents::status::IrisStatus::analysis());
        let intelligent_context = self.gather_intelligent_context(context).await?;

        // Step 2: Build Git context from intelligent analysis
        crate::agents::status::IRIS_STATUS.update(crate::agents::status::IrisStatus::synthesis());
        let commit_context = self
            .build_commit_context(context, &intelligent_context)
            .await?;

        // Step 3: Create prompts
        crate::agents::status::IRIS_STATUS.update(crate::agents::status::IrisStatus::planning());
        let mut config_clone = (*context.config).clone();
        config_clone.use_gitmoji = use_gitmoji;

        let system_prompt = create_system_prompt(&config_clone)?;
        let user_prompt = create_user_prompt(&commit_context);

        // Step 4: Generate with LLM using streaming
        crate::agents::status::IRIS_STATUS.update(crate::agents::status::IrisStatus::generation());
        let generated_message = self
            .generate_with_backend_streaming(&system_prompt, &user_prompt, callback)
            .await?;

        // Step 5: Parse and validate response
        crate::agents::status::IRIS_STATUS.update(crate::agents::status::IrisStatus::synthesis());
        let parsed_response = self.parse_json_response::<GeneratedMessage>(&generated_message)?;

        log_debug!(
            "‚úÖ Iris: Streaming commit message generated - Title: '{}', {} chars total",
            parsed_response.title,
            parsed_response.message.len()
        );

        iris_status_completed!();
        Ok(TaskResult::success_with_data(
            "Commit message generated successfully with streaming".to_string(),
            serde_json::to_value(parsed_response)?,
        )
        .with_confidence(0.92))
    }

    /// Generate a code review with intelligent analysis
    pub async fn generate_code_review(
        &self,
        context: &AgentContext,
        _params: &HashMap<String, serde_json::Value>,
    ) -> Result<TaskResult> {
        log_debug!("üîç Iris: Starting intelligent code review");

        // Gather intelligent context
        let intelligent_context = self.gather_intelligent_context(context).await?;
        let commit_context = self
            .build_commit_context(context, &intelligent_context)
            .await?;

        // Generate review using existing prompt system with intelligent context management
        let system_prompt = create_review_system_prompt(&context.config)?;
        let managed_user_prompt = self.manage_review_context(&commit_context).await?;

        let generated_review = self
            .generate_with_backend(&system_prompt, &managed_user_prompt)
            .await?;
        let parsed_response = self.parse_json_response::<GeneratedReview>(&generated_review)?;

        log_debug!(
            "‚úÖ Iris: Code review completed with {} issues",
            parsed_response.issues.len()
        );

        Ok(TaskResult::success_with_data(
            "Code review generated successfully".to_string(),
            serde_json::to_value(parsed_response)?,
        )
        .with_confidence(0.88))
    }

    /// Generate a pull request description with intelligent analysis
    pub async fn generate_pull_request(
        &self,
        context: &AgentContext,
        params: &HashMap<String, serde_json::Value>,
    ) -> Result<TaskResult> {
        log_debug!("üìã Iris: Starting pull request description generation");

        // Get commit messages from params
        let commit_messages = params
            .get("commit_messages")
            .and_then(|v| v.as_array())
            .map(|arr| {
                arr.iter()
                    .filter_map(|v| v.as_str())
                    .map(std::string::ToString::to_string)
                    .collect::<Vec<_>>()
            })
            .unwrap_or_default();

        // Gather intelligent context
        let intelligent_context = self.gather_intelligent_context(context).await?;
        let commit_context = self
            .build_commit_context(context, &intelligent_context)
            .await?;

        // Generate PR description using existing prompt system
        let system_prompt = create_pr_system_prompt(&context.config)?;
        let user_prompt = create_pr_user_prompt(&commit_context, &commit_messages);

        let generated_pr = self
            .generate_with_backend(&system_prompt, &user_prompt)
            .await?;
        let parsed_response = self.parse_json_response::<GeneratedPullRequest>(&generated_pr)?;

        log_debug!(
            "‚úÖ Iris: PR description generated - Title: '{}'",
            parsed_response.title
        );

        Ok(TaskResult::success_with_data(
            "Pull request description generated successfully".to_string(),
            serde_json::to_value(parsed_response)?,
        )
        .with_confidence(0.90))
    }

    /// Gather intelligent context using LLM-driven tool selection and usage
    async fn gather_intelligent_context(
        &self,
        context: &AgentContext,
    ) -> Result<IntelligentContext> {
        log_debug!(
            "üß† Iris: Starting intelligent context analysis with agent-driven tool selection"
        );

        // Step 1: Let the agent decide which tools to use
        log_debug!("ü§ñ Iris: Planning tool usage strategy");
        iris_status_planning!();
        let tool_plan = self.plan_tool_usage_for_context_analysis(context).await?;
        log_debug!("üìã Iris: Agent planned {} tool operations", tool_plan.len());

        // Step 2: Execute the planned tool calls
        log_debug!("üîß Iris: Executing agent-planned tool calls");
        let tool_results = self.execute_planned_tool_calls(context, &tool_plan).await?;
        log_debug!("‚úÖ Iris: Completed {} tool operations", tool_results.len());

        // Step 3: Use LLM to synthesize tool results into intelligent context
        log_debug!("ü§ñ Iris: Synthesizing tool results with LLM");
        iris_status_synthesis!();
        let synthesis_prompt = self.build_tool_synthesis_prompt(&tool_results)?;
        log_debug!(
            "üõ†Ô∏è Iris: Synthesis prompt built - {} chars",
            synthesis_prompt.len()
        );

        let intelligence_result = self.analyze_with_backend(&synthesis_prompt).await?;
        log_debug!(
            "ü§ñ Iris: LLM synthesis response received - {} chars",
            intelligence_result.len()
        );

        // Step 4: Parse the synthesized analysis
        log_debug!("üîç Iris: Parsing synthesized analysis result");
        iris_status_analysis!();

        // For now, fall back to basic git context to maintain compatibility
        // TODO: Build IntelligentContext entirely from tool synthesis
        let git_context = context.git_repo.get_git_info(&context.config).await?;
        let intelligent_context =
            self.parse_intelligence_result(&intelligence_result, &git_context)?;

        log_debug!(
            "‚úÖ Iris: Intelligent context gathered via agent-driven tools - {} files analyzed",
            intelligent_context.files_with_relevance.len()
        );

        Ok(intelligent_context)
    }

    /// Let the agent plan which tools to use for context analysis
    async fn plan_tool_usage_for_context_analysis(
        &self,
        _context: &AgentContext,
    ) -> Result<Vec<ToolPlan>> {
        log_debug!("ü§ñ Iris: Agent planning tool usage strategy");

        // Get available tools
        let available_tools: Vec<String> = self
            .tools
            .iter()
            .map(|tool| format!("{}: {}", tool.name(), tool.description()))
            .collect();

        log_debug!("üîß Iris: Available tools: {}", available_tools.len());

        // Create initial tool planning prompt
        let planning_prompt = format!(
            "You are Iris, an intelligent AI assistant specialized in Git workflow automation and analysis. Create an initial plan for tools to use to gather comprehensive context.\n\n\
            Your Task: Analyze Git changes to understand their purpose, impact, and relevance for generating a high-quality commit message.\n\n\
            Available tools at your disposal:\n{}\n\n\
            Repository context:\n\
            - This is a Git repository with staged changes\n\
            - You need to understand what changed and why\n\
            - You should gather enough context to assess file relevance and change impact\n\
            - You can expand or adjust this plan as you discover more context\n\n\
            As Iris, respond with a JSON array of tool calls in the order you want to execute them:\n\
            [\n\
              {{\n\
                \"tool_name\": \"Git Operations\",\n\
                \"operation\": \"diff\",\n\
                \"parameters\": {{}},\n\
                \"reason\": \"I need to see the actual code changes to understand what was modified\",\n\
                \"priority\": \"high\"\n\
              }}\n\
            ]\n\n\
            Start with 2-3 essential tool calls. You can expand your plan based on what you discover.",
            available_tools.join("\n")
        );

        log_debug!("ü§ñ Iris: Sending tool planning request to LLM");
        let planning_result = self.analyze_with_backend(&planning_prompt).await?;
        log_debug!(
            "üìã Iris: Tool planning response received - {} chars",
            planning_result.len()
        );

        // Parse the tool plan
        let tool_plan = self.parse_tool_plan(&planning_result)?;
        log_debug!(
            "üìã Iris: Parsed {} planned tool operations",
            tool_plan.len()
        );

        Ok(tool_plan)
    }

    /// Execute the planned tool calls with dynamic plan adjustment
    async fn execute_planned_tool_calls(
        &self,
        context: &AgentContext,
        initial_plan: &[ToolPlan],
    ) -> Result<Vec<ToolResult>> {
        let mut results = Vec::new();
        let mut current_plan = initial_plan.to_vec();
        let mut executed_tools = std::collections::HashSet::new();

        log_debug!(
            "üîß Iris: Starting adaptive tool execution with {} initial tools",
            current_plan.len()
        );

        while !current_plan.is_empty() {
            // Execute the next tool in the plan
            let plan = current_plan.remove(0);
            let plan_key = format!(
                "{}:{}",
                plan.tool_name,
                plan.operation.as_deref().unwrap_or("default")
            );

            // Skip if we've already executed this exact tool+operation combination
            if executed_tools.contains(&plan_key) {
                log_debug!("‚è≠Ô∏è Iris: Skipping already executed tool: {}", plan_key);
                continue;
            }

            log_debug!(
                "üîß Iris: Executing tool: {} ({})",
                plan.tool_name,
                plan.reason
            );
            iris_status_tool!(&plan.tool_name, &plan.reason);

            // Find the tool by name
            let tool = self
                .tools
                .iter()
                .find(|t| t.name() == plan.tool_name)
                .ok_or_else(|| anyhow::anyhow!("Tool not found: {}", plan.tool_name))?;

            // Execute the tool with planned parameters
            let mut params = plan.parameters.clone();
            if let Some(operation) = &plan.operation {
                params.insert(
                    "operation".to_string(),
                    serde_json::Value::String(operation.clone()),
                );
            }

            match tool.execute(context, &params).await {
                Ok(result) => {
                    log_debug!("‚úÖ Iris: Tool call completed: {}", plan.tool_name);
                    executed_tools.insert(plan_key);

                    let tool_result = ToolResult {
                        tool_name: plan.tool_name.clone(),
                        operation: plan.operation.clone(),
                        result,
                        reason: plan.reason.clone(),
                    };

                    results.push(tool_result.clone());

                    // After each tool execution, check if we need to expand the plan
                    if results.len() <= 2 {
                        // Only expand during early execution
                        log_debug!(
                            "ü§ñ Iris: Checking if plan needs expansion based on new context"
                        );
                        iris_status_expansion!();
                        let expanded_plan = self
                            .expand_plan_based_on_context(context, &results, &current_plan)
                            .await?;
                        if !expanded_plan.is_empty() {
                            log_debug!(
                                "üìã Iris: Plan expanded with {} additional tools",
                                expanded_plan.len()
                            );
                            current_plan.extend(expanded_plan);
                        }
                    }
                }
                Err(e) => {
                    log_debug!("‚ùå Iris: Tool call failed for {}: {}", plan.tool_name, e);
                    // Continue with other tools even if one fails
                }
            }
        }

        log_debug!(
            "üéØ Iris: Completed {} tool executions through adaptive planning",
            results.len()
        );
        Ok(results)
    }

    /// Expand the plan based on discovered context
    async fn expand_plan_based_on_context(
        &self,
        _context: &AgentContext,
        current_results: &[ToolResult],
        remaining_plan: &[ToolPlan],
    ) -> Result<Vec<ToolPlan>> {
        // Analyze current results to determine if we need more tools
        let mut context_summary = String::new();
        for result in current_results {
            writeln!(
                context_summary,
                "Tool '{}' revealed: {}",
                result.tool_name,
                match &result.result {
                    serde_json::Value::Object(obj) => {
                        obj.get("content").and_then(|v| v.as_str()).map_or_else(
                            || "Complex data structure".to_string(),
                            |s| s.chars().take(200).collect::<String>(),
                        )
                    }
                    _ => "Non-object result".to_string(),
                }
            )
            .unwrap();
        }

        // Get available tools not in current plan
        let planned_tools: std::collections::HashSet<String> =
            remaining_plan.iter().map(|p| p.tool_name.clone()).collect();
        let available_tools: Vec<String> = self
            .tools
            .iter()
            .filter(|tool| !planned_tools.contains(tool.name()))
            .map(|tool| format!("{}: {}", tool.name(), tool.description()))
            .collect();

        if available_tools.is_empty() {
            log_debug!("ü§ñ Iris: No additional tools available for plan expansion");
            return Ok(Vec::new());
        }

        let expansion_prompt = format!(
            "You are Iris, analyzing the context you've discovered so far. Based on what you've learned, \
            determine if you need additional tools to get a complete understanding.\n\n\
            Context you've discovered so far:\n{}\n\n\
            Your remaining planned tools:\n{}\n\n\
            Additional tools available to you:\n{}\n\n\
            As Iris, should you add more tools to your plan? If yes, respond with a JSON array of additional tool calls. \
            If you have enough context, respond with an empty array [].\n\n\
            Focus on tools that will provide missing context or deeper analysis for your understanding.",
            context_summary,
            remaining_plan
                .iter()
                .map(|p| format!("{} ({})", p.tool_name, p.reason))
                .collect::<Vec<_>>()
                .join(", "),
            available_tools.join("\n")
        );

        log_debug!("ü§ñ Iris: Requesting plan expansion from LLM");
        let expansion_result = self.analyze_with_backend(&expansion_prompt).await?;
        log_debug!(
            "üìã Iris: Plan expansion response received - {} chars",
            expansion_result.len()
        );

        let expanded_tools = self.parse_tool_plan(&expansion_result)?;
        log_debug!(
            "üìã Iris: Parsed {} additional tools for plan expansion",
            expanded_tools.len()
        );

        Ok(expanded_tools)
    }

    /// Build prompt to synthesize tool results into intelligent context
    #[allow(clippy::unused_self, clippy::unnecessary_wraps)]
    fn build_tool_synthesis_prompt(&self, tool_results: &[ToolResult]) -> Result<String> {
        let mut prompt = String::from(
            "You are Iris, an expert AI assistant synthesizing information from multiple tools to understand Git changes.\n\n\
            Your task is to analyze the tool results and provide intelligent insights about file relevance, change purpose, and overall impact.\n\n\
            Tool Results:\n\n",
        );

        for (i, result) in tool_results.iter().enumerate() {
            write!(
                prompt,
                "=== TOOL RESULT {} ===\n\
                Tool: {}\n\
                Operation: {}\n\
                Purpose: {}\n\
                Result:\n{}\n\n",
                i + 1,
                result.tool_name,
                result.operation.as_deref().unwrap_or("N/A"),
                result.reason,
                serde_json::to_string_pretty(&result.result)
                    .unwrap_or_else(|_| "Unable to serialize".to_string())
            )
            .unwrap();
        }

        prompt.push_str(
            "Based on these tool results, as Iris provide your analysis in this JSON format:\n\
            {\n\
              \"files\": [\n\
                {\n\
                  \"path\": \"file_path\",\n\
                  \"relevance_score\": 0.85,\n\
                  \"analysis\": \"What changed and why it matters\",\n\
                  \"key_changes\": [\"change 1\", \"change 2\"],\n\
                  \"impact_assessment\": \"How this affects the system\"\n\
                }\n\
              ],\n\
              \"change_summary\": \"Overall purpose of these changes\",\n\
              \"technical_analysis\": \"Implementation details and patterns\",\n\
              \"project_insights\": \"How this fits into the larger codebase\"\n\
            }",
        );

        Ok(prompt)
    }

    /// Parse tool planning response into structured tool plan
    #[allow(clippy::unused_self, clippy::unnecessary_wraps)]
    fn parse_tool_plan(&self, planning_result: &str) -> Result<Vec<ToolPlan>> {
        log_debug!("üìã Iris: Parsing tool plan from LLM response");

        // Try to parse JSON response
        if let Ok(parsed) = serde_json::from_str::<serde_json::Value>(planning_result) {
            if let Some(array) = parsed.as_array() {
                let mut tool_plan = Vec::new();

                for item in array {
                    if let (Some(tool_name), Some(reason)) = (
                        item.get("tool_name").and_then(|v| v.as_str()),
                        item.get("reason").and_then(|v| v.as_str()),
                    ) {
                        let operation = item
                            .get("operation")
                            .and_then(|v| v.as_str())
                            .map(std::string::ToString::to_string);
                        let parameters = item
                            .get("parameters")
                            .and_then(|v| v.as_object())
                            .map(|obj| obj.iter().map(|(k, v)| (k.clone(), v.clone())).collect())
                            .unwrap_or_default();

                        tool_plan.push(ToolPlan {
                            tool_name: tool_name.to_string(),
                            operation,
                            parameters,
                            reason: reason.to_string(),
                        });
                    }
                }

                log_debug!(
                    "‚úÖ Iris: Successfully parsed {} tool operations",
                    tool_plan.len()
                );
                return Ok(tool_plan);
            }
        }

        // Fallback: create basic tool plan if parsing fails
        log_debug!("‚ö†Ô∏è Iris: Failed to parse tool plan, using fallback");
        Ok(vec![ToolPlan {
            tool_name: "Git Operations".to_string(),
            operation: Some("diff".to_string()),
            parameters: std::collections::HashMap::new(),
            reason: "Get code changes for analysis".to_string(),
        }])
    }

    /// Parse LLM intelligence result into structured context
    #[allow(clippy::unused_self, clippy::unnecessary_wraps, clippy::too_many_lines)]
    fn parse_intelligence_result(
        &self,
        result: &str,
        git_context: &CommitContext,
    ) -> Result<IntelligentContext> {
        log_debug!(
            "üîç Iris: Parsing LLM analysis result: {}",
            result.chars().take(200).collect::<String>()
        );

        // Try to parse JSON response
        log_debug!("üîç Iris: Attempting to parse JSON response");
        if let Ok(parsed) = serde_json::from_str::<serde_json::Value>(result) {
            log_debug!("‚úÖ Iris: Successfully parsed LLM analysis JSON");

            let files_with_relevance: Vec<FileRelevance> = parsed
                .get("files")
                .and_then(|f| f.as_array())
                .map(|files| {
                    log_debug!("üìä Iris: Processing {} file analyses from LLM", files.len());
                    files
                        .iter()
                        .enumerate()
                        .filter_map(|(i, file)| {
                            let file_result = Some(FileRelevance {
                                path: file.get("path")?.as_str()?.to_string(),
                                #[allow(clippy::cast_possible_truncation, clippy::as_conversions)]
                                relevance_score: file.get("relevance_score")?.as_f64()? as f32,
                                analysis: file.get("analysis")?.as_str()?.to_string(),
                                key_changes: file
                                    .get("key_changes")?
                                    .as_array()?
                                    .iter()
                                    .filter_map(|v| {
                                        v.as_str().map(std::string::ToString::to_string)
                                    })
                                    .collect(),
                                impact_assessment: file
                                    .get("impact_assessment")?
                                    .as_str()?
                                    .to_string(),
                            });

                            if let Some(ref fr) = file_result {
                                log_debug!(
                                    "üìÑ Iris: File {} analysis - relevance: {:.2}, {} key changes",
                                    fr.path,
                                    fr.relevance_score,
                                    fr.key_changes.len()
                                );
                            } else {
                                log_debug!("‚ö†Ô∏è Iris: Failed to parse file analysis #{}", i + 1);
                            }

                            file_result
                        })
                        .collect()
                })
                .unwrap_or_default();

            let change_summary = parsed
                .get("change_summary")
                .and_then(|v| v.as_str())
                .unwrap_or("Changes analyzed")
                .to_string();

            let technical_analysis = parsed
                .get("technical_analysis")
                .and_then(|v| v.as_str())
                .unwrap_or("Technical implementation details")
                .to_string();

            let project_insights = parsed
                .get("project_insights")
                .and_then(|v| v.as_str())
                .unwrap_or("Project context and fit")
                .to_string();

            log_debug!(
                "‚úÖ Iris: Successfully parsed intelligent context with {} file analyses",
                files_with_relevance.len()
            );
            log_debug!(
                "üìù Iris: Change summary: {}",
                change_summary.chars().take(100).collect::<String>()
            );

            Ok(IntelligentContext {
                files_with_relevance,
                change_summary,
                technical_analysis,
                project_insights,
            })
        } else {
            // Fallback: create basic context with equal relevance
            log_debug!("‚ö†Ô∏è Iris: Failed to parse LLM analysis JSON, using fallback context");
            log_debug!(
                "‚ö†Ô∏è Iris: JSON parsing error - raw response: {}",
                result.chars().take(500).collect::<String>()
            );

            let files_with_relevance: Vec<FileRelevance> = git_context
                .staged_files
                .iter()
                .enumerate()
                .map(|(i, file)| {
                    log_debug!(
                        "üìÑ Iris: Creating fallback analysis for file {}: {}",
                        i + 1,
                        file.path
                    );
                    FileRelevance {
                        path: file.path.clone(),
                        relevance_score: 0.7, // Default relevance
                        analysis: format!("File {} was modified", file.path),
                        key_changes: vec!["Content changes detected".to_string()],
                        impact_assessment: "Part of the overall changeset".to_string(),
                    }
                })
                .collect();

            log_debug!(
                "‚ö†Ô∏è Iris: Created fallback context with {} files",
                files_with_relevance.len()
            );

            Ok(IntelligentContext {
                files_with_relevance,
                change_summary: "Multiple files changed".to_string(),
                technical_analysis: "Various technical changes applied".to_string(),
                project_insights: "Changes contribute to project evolution".to_string(),
            })
        }
    }

    /// Build commit context from intelligent analysis
    async fn build_commit_context(
        &self,
        context: &AgentContext,
        intelligent_context: &IntelligentContext,
    ) -> Result<CommitContext> {
        log_debug!("üèóÔ∏è Iris: Building commit context from intelligent analysis");

        let git_context = context.git_repo.get_git_info(&context.config).await?;
        log_debug!(
            "üèóÔ∏è Iris: Git context retrieved with {} staged files",
            git_context.staged_files.len()
        );

        // The git_context already contains the staged files we need
        // We just need to enhance them with intelligent analysis
        let mut staged_files = git_context.staged_files.clone();

        // Enhance with intelligent analysis
        let mut enhanced_count = 0;
        for staged_file in &mut staged_files {
            if let Some(relevance_info) = intelligent_context
                .files_with_relevance
                .iter()
                .find(|f| f.path == staged_file.path)
            {
                // Replace analysis with intelligent insights
                staged_file.analysis = relevance_info.key_changes.clone();
                enhanced_count += 1;
                log_debug!(
                    "üîß Iris: Enhanced file {} with {} key changes (relevance: {:.2})",
                    staged_file.path,
                    relevance_info.key_changes.len(),
                    relevance_info.relevance_score
                );
            } else {
                log_debug!(
                    "‚ö†Ô∏è Iris: No intelligent analysis found for file: {}",
                    staged_file.path
                );
            }
        }

        log_debug!(
            "üèóÔ∏è Iris: Enhanced {} of {} files with intelligent analysis",
            enhanced_count,
            staged_files.len()
        );

        // Return the enhanced git_context with intelligent analysis
        Ok(CommitContext {
            branch: git_context.branch,
            staged_files,
            recent_commits: git_context.recent_commits,
            project_metadata: git_context.project_metadata,
            user_name: git_context.user_name,
            user_email: git_context.user_email,
        })
    }

    /// Generate text using Rig's optimized agent builder pattern
    async fn generate_with_backend(
        &self,
        system_prompt: &str,
        user_prompt: &str,
    ) -> Result<String> {
        log_debug!("üîÆ Iris: Preparing optimized LLM request with Rig");
        log_debug!("üìä System prompt: {} chars", system_prompt.len());
        log_debug!("üë§ User prompt: {} chars", user_prompt.len());

        // Use Rig's fluent builder pattern for optimal configuration
        let response_text = match &self.backend {
            AgentBackend::OpenAI { client, model } => {
                log_debug!("ü§ñ Using Rig-optimized OpenAI agent builder");

                // Leverage Rig's chainable configuration for generation tasks
                let response = client
                    .agent(model)
                    .preamble(system_prompt)
                    .temperature(0.7)
                    .max_tokens(DEFAULT_MAX_TOKENS)
                    .build()
                    .prompt(user_prompt)
                    .await
                    .map_err(|e| anyhow::anyhow!("OpenAI API error: {}", e))?;

                log_debug!("‚úÖ OpenAI response received via Rig");
                response
            }
            AgentBackend::Anthropic { client, model } => {
                log_debug!("üß† Using Rig-optimized Anthropic agent builder");

                // Leverage Rig's unified API for different providers
                let response = client
                    .agent(model)
                    .preamble(system_prompt)
                    .temperature(0.7)
                    .max_tokens(DEFAULT_MAX_TOKENS)
                    .build()
                    .prompt(user_prompt)
                    .await
                    .map_err(|e| anyhow::anyhow!("Anthropic API error: {}", e))?;

                log_debug!("‚úÖ Anthropic response received via Rig");
                response
            }
        };

        log_debug!("‚úÖ Response processed: {} chars", response_text.len());
        log_debug!(
            "üìù Response preview: {}",
            response_text.chars().take(100).collect::<String>()
        );

        Ok(response_text.trim().to_string())
    }

    /// Generate text using Rig's streaming interface for real-time feedback
    async fn generate_with_backend_streaming(
        &self,
        system_prompt: &str,
        user_prompt: &str,
        callback: &dyn StreamingCallback,
    ) -> Result<String> {
        log_debug!("üåä Iris: Preparing streaming LLM request with Rig");
        log_debug!("üìä System prompt: {} chars", system_prompt.len());
        log_debug!("üë§ User prompt: {} chars", user_prompt.len());

        let mut full_response = String::new();

        // Use Rig's streaming capabilities
        let result = match &self.backend {
            AgentBackend::OpenAI { client, model } => {
                log_debug!("ü§ñ Using Rig streaming for OpenAI");

                // Create streaming agent
                let agent = client
                    .agent(model)
                    .preamble(system_prompt)
                    .temperature(0.7)
                    .max_tokens(DEFAULT_MAX_TOKENS)
                    .build();

                // Use Rig's real streaming API
                match agent.stream_prompt(user_prompt).await {
                    Ok(mut stream) => {
                        // Process stream chunks as they arrive
                        while let Some(chunk_result) = stream.next().await {
                            match chunk_result {
                                Ok(assistant_content) => {
                                    // Extract text from AssistantContent
                                    match assistant_content {
                                        rig::completion::AssistantContent::Text(text) => {
                                            if !text.text.is_empty() {
                                                callback.on_chunk(&text.text).await?;
                                                full_response.push_str(&text.text);
                                            }
                                        }
                                        rig::completion::AssistantContent::ToolCall(_) => {
                                            // Handle tool calls if needed
                                            log_debug!(
                                                "üîß Received tool call in streaming response"
                                            );
                                        }
                                    }
                                }
                                Err(e) => {
                                    let error = anyhow::anyhow!("OpenAI streaming error: {}", e);
                                    callback.on_error(&error).await?;
                                    return Err(error);
                                }
                            }
                        }

                        // The full response is accumulated in full_response
                        Ok(full_response.clone())
                    }
                    Err(e) => {
                        let error = anyhow::anyhow!("OpenAI streaming error: {}", e);
                        callback.on_error(&error).await?;
                        Err(error)
                    }
                }
            }
            AgentBackend::Anthropic { client, model } => {
                log_debug!("üß† Using Rig streaming for Anthropic");

                // Create streaming agent
                let agent = client
                    .agent(model)
                    .preamble(system_prompt)
                    .temperature(0.7)
                    .max_tokens(DEFAULT_MAX_TOKENS)
                    .build();

                // Use Rig's real streaming API
                match agent.stream_prompt(user_prompt).await {
                    Ok(mut stream) => {
                        // Process stream chunks as they arrive
                        while let Some(chunk_result) = stream.next().await {
                            match chunk_result {
                                Ok(assistant_content) => {
                                    // Extract text from AssistantContent
                                    match assistant_content {
                                        rig::completion::AssistantContent::Text(text) => {
                                            if !text.text.is_empty() {
                                                callback.on_chunk(&text.text).await?;
                                                full_response.push_str(&text.text);
                                            }
                                        }
                                        rig::completion::AssistantContent::ToolCall(_) => {
                                            // Handle tool calls if needed
                                            log_debug!(
                                                "üîß Received tool call in streaming response"
                                            );
                                        }
                                    }
                                }
                                Err(e) => {
                                    let error = anyhow::anyhow!("Anthropic streaming error: {}", e);
                                    callback.on_error(&error).await?;
                                    return Err(error);
                                }
                            }
                        }

                        // The full response is accumulated in full_response
                        Ok(full_response.clone())
                    }
                    Err(e) => {
                        let error = anyhow::anyhow!("Anthropic streaming error: {}", e);
                        callback.on_error(&error).await?;
                        Err(error)
                    }
                }
            }
        };

        match result {
            Ok(response) => {
                callback.on_complete(&response).await?;
                log_debug!("‚úÖ Streaming response completed: {} chars", response.len());
                Ok(response.trim().to_string())
            }
            Err(e) => Err(e),
        }
    }

    /// Analyze context using Rig's optimized analysis configuration
    async fn analyze_with_backend(&self, prompt: &str) -> Result<String> {
        let system_prompt = "You are Iris, an expert AI assistant specializing in Git workflow automation and code analysis. \
                            Provide intelligent, structured analysis in the requested JSON format. \
                            You have deep understanding of software development patterns and can provide insightful analysis.";

        log_debug!("ü§ñ Iris: Preparing Rig-optimized intelligence analysis");
        log_debug!("üìä Analysis prompt: {} chars", prompt.len());

        // Use Rig's optimized configuration for analysis tasks
        let response_text = match &self.backend {
            AgentBackend::OpenAI { client, model } => {
                log_debug!("ü§ñ Using Rig-optimized OpenAI analysis configuration");

                // Configure agent specifically for analysis with Rig's builder pattern
                let response = client
                    .agent(model)
                    .preamble(system_prompt)
                    .temperature(0.3) // Lower temperature for consistent analysis
                    .max_tokens(DEFAULT_MAX_TOKENS)
                    .build()
                    .prompt(prompt)
                    .await
                    .map_err(|e| anyhow::anyhow!("OpenAI analysis error: {}", e))?;

                log_debug!("‚úÖ OpenAI analysis completed via Rig");
                response
            }
            AgentBackend::Anthropic { client, model } => {
                log_debug!("üß† Using Rig-optimized Anthropic analysis configuration");

                // Leverage Rig's provider abstraction for consistent interface
                let response = client
                    .agent(model)
                    .preamble(system_prompt)
                    .temperature(0.3)
                    .max_tokens(DEFAULT_MAX_TOKENS)
                    .build()
                    .prompt(prompt)
                    .await
                    .map_err(|e| anyhow::anyhow!("Anthropic analysis error: {}", e))?;

                log_debug!("‚úÖ Anthropic analysis completed via Rig");
                response
            }
        };

        log_debug!(
            "‚úÖ Intelligence analysis response received: {} chars",
            response_text.len()
        );
        log_debug!(
            "üìù Analysis response preview: {}",
            response_text.chars().take(200).collect::<String>()
        );

        Ok(response_text.trim().to_string())
    }

    /// Parse JSON response with fallback handling
    #[allow(clippy::unused_self, clippy::cognitive_complexity)]
    fn parse_json_response<T>(&self, response: &str) -> Result<T>
    where
        T: for<'de> Deserialize<'de>,
    {
        log_debug!("üîç Iris: Parsing JSON response - {} chars", response.len());
        log_debug!(
            "üìù Iris: Response preview: {}",
            response.chars().take(200).collect::<String>()
        );

        // First try to parse the response directly
        log_debug!("üéØ Iris: Attempting direct JSON parsing");
        if let Ok(parsed) = serde_json::from_str::<T>(response) {
            log_debug!("‚úÖ Iris: Direct JSON parsing successful");
            return Ok(parsed);
        }
        log_debug!("‚ùå Iris: Direct JSON parsing failed, trying markdown extraction");

        // Try to extract JSON from markdown code blocks
        if let Some(json_start) = response.find("```json") {
            log_debug!(
                "üîç Iris: Found markdown JSON block at position {}",
                json_start
            );
            let content_start = json_start + 7; // Skip past "```json"
            if let Some(json_end_relative) = response[content_start..].find("```") {
                let json_end = content_start + json_end_relative;
                let json_content = &response[content_start..json_end];
                log_debug!(
                    "üìÑ Iris: Extracted JSON content - {} chars",
                    json_content.trim().len()
                );
                if let Ok(parsed) = serde_json::from_str::<T>(json_content.trim()) {
                    log_debug!("‚úÖ Iris: Markdown JSON parsing successful");
                    return Ok(parsed);
                }
                log_debug!("‚ùå Iris: Markdown JSON parsing failed");
            } else {
                log_debug!("‚ùå Iris: Found ```json but no closing ```");
            }
        } else {
            log_debug!("üîç Iris: No markdown JSON blocks found");
        }

        // Try to find any JSON object in the response
        if let Some(start) = response.find('{') {
            if let Some(end) = response.rfind('}') {
                let potential_json = &response[start..=end];
                log_debug!(
                    "üîç Iris: Found potential JSON object from {} to {} - {} chars",
                    start,
                    end,
                    potential_json.len()
                );
                log_debug!(
                    "üìÑ Iris: Potential JSON preview: {}",
                    potential_json.chars().take(100).collect::<String>()
                );
                if let Ok(parsed) = serde_json::from_str::<T>(potential_json) {
                    log_debug!("‚úÖ Iris: Extracted JSON parsing successful");
                    return Ok(parsed);
                }
                log_debug!("‚ùå Iris: Extracted JSON parsing failed");
            } else {
                log_debug!("‚ùå Iris: Found opening {{ but no closing }}");
            }
        } else {
            log_debug!("‚ùå Iris: No JSON objects found in response");
        }

        // Last resort: try to handle truncated JSON by finding the last complete field
        if let Some(start) = response.find('{') {
            // Find the last complete field by looking for the last closing brace before any truncation
            let mut brace_count = 0;
            let mut last_valid_end = start;

            for (i, c) in response[start..].char_indices() {
                match c {
                    '{' => brace_count += 1,
                    '}' => {
                        brace_count -= 1;
                        if brace_count == 0 {
                            last_valid_end = start + i;
                            break;
                        }
                    }
                    _ => {}
                }
            }

            if last_valid_end > start {
                let truncated_json = &response[start..=last_valid_end];
                log_debug!(
                    "üîß Iris: Attempting to parse truncated JSON - {} chars",
                    truncated_json.len()
                );
                if let Ok(parsed) = serde_json::from_str::<T>(truncated_json) {
                    log_debug!("‚úÖ Iris: Truncated JSON parsing successful");
                    return Ok(parsed);
                }
            }
        }

        log_debug!("üö® Iris: All JSON parsing attempts failed");
        Err(anyhow::anyhow!(
            "Failed to parse JSON response. Raw response: {}",
            response.chars().take(1000).collect::<String>() // Limit error message length
        ))
    }

    /// Generate changelog with advanced features like file updating and version management
    async fn generate_changelog(
        &self,
        context: &AgentContext,
        params: &HashMap<String, serde_json::Value>,
    ) -> Result<TaskResult> {
        log_debug!("üìú Iris: Starting comprehensive changelog generation");

        // Extract parameters
        let version = params
            .get("version")
            .and_then(|v| v.as_str())
            .unwrap_or("Latest");
        let from_tag = params.get("from_tag").and_then(|v| v.as_str());
        let to_tag = params.get("to_tag").and_then(|v| v.as_str());
        let update_file = params
            .get("update_file")
            .and_then(serde_json::Value::as_bool)
            .unwrap_or(false);
        let file_path = params
            .get("file_path")
            .and_then(|v| v.as_str())
            .unwrap_or("CHANGELOG.md");

        // Step 1: Gather git context for the changelog range
        iris_status_analysis!();
        let git_context = self
            .gather_git_changelog_context(context, from_tag, to_tag)
            .await?;

        // Step 2: Generate changelog content using LLM
        iris_status_generation!();
        let changelog_content = self
            .generate_changelog_content(context, &git_context, version)
            .await?;

        // Step 3: Apply bordered formatting like the original
        let bordered_content = self.add_changelog_borders(&changelog_content);

        // Step 4: Update file if requested
        if update_file {
            iris_status_synthesis!();
            self.update_changelog_file(
                &changelog_content,
                file_path,
                context,
                to_tag,
                Some(version.to_string()),
            )
            .await?;
        }

        log_debug!("‚úÖ Iris: Changelog generation completed");

        Ok(TaskResult::success_with_data(
            "Changelog generated successfully".to_string(),
            serde_json::json!({
                "formatted": bordered_content,
                "version": version,
                "updated_file": update_file,
                "file_path": file_path
            }),
        ))
    }

    /// Generate release notes
    async fn generate_release_notes(
        &self,
        context: &AgentContext,
        params: &HashMap<String, serde_json::Value>,
    ) -> Result<TaskResult> {
        log_debug!("üéâ Iris: Starting release notes generation");

        // Extract parameters
        let version = params
            .get("version")
            .and_then(|v| v.as_str())
            .unwrap_or("Latest");
        let from_tag = params.get("from_tag").and_then(|v| v.as_str());
        let to_tag = params.get("to_tag").and_then(|v| v.as_str());

        // Step 1: Gather git context
        iris_status_analysis!();
        let git_context = self
            .gather_git_changelog_context(context, from_tag, to_tag)
            .await?;

        // Step 2: Generate release notes content
        iris_status_generation!();
        let release_notes = self
            .generate_release_notes_content(context, &git_context, version)
            .await?;

        // Step 3: Apply bordered formatting like the original
        let bordered_content = self.add_changelog_borders(&release_notes);

        log_debug!("‚úÖ Iris: Release notes generation completed");

        Ok(TaskResult::success_with_data(
            "Release notes generated successfully".to_string(),
            serde_json::json!({
                "formatted": bordered_content,
                "version": version
            }),
        ))
    }

    /// Gather git context for changelog/release notes generation
    async fn gather_git_changelog_context(
        &self,
        context: &AgentContext,
        from_tag: Option<&str>,
        to_tag: Option<&str>,
    ) -> Result<String> {
        log_debug!("üìä Iris: Gathering git context for changelog");

        let from_ref = from_tag.unwrap_or("HEAD~10"); // Default to last 10 commits
        let to_ref = to_tag.unwrap_or("HEAD");

        // Get commit range data
        let commits = context
            .git_repo
            .get_commits_for_pr(from_ref, to_ref)
            .unwrap_or_else(|_| vec!["No commits available".to_string()]);

        let files = context
            .git_repo
            .get_commit_range_files(from_ref, to_ref)
            .unwrap_or_else(|_| vec![]);

        let commits_text = commits.join("\n");
        let files_text = files
            .iter()
            .map(|f| format!("{}: {}", f.change_type, f.path))
            .collect::<Vec<_>>()
            .join("\n");

        let context_text = format!(
            "Commit Range: {from_ref} to {to_ref}\n\nCommits:\n{commits_text}\n\nFiles Changed:\n{files_text}"
        );

        Ok(context_text)
    }

    /// Generate changelog content using LLM with the original structured format
    async fn generate_changelog_content(
        &self,
        context: &AgentContext,
        git_context: &str,
        version: &str,
    ) -> Result<String> {
        // Use the original changelog system prompt for consistency
        let system_prompt = self.create_original_changelog_system_prompt(context);

        let user_prompt = format!(
            "Generate a structured changelog JSON for version {version} based on the following git context:\n\n{git_context}\n\n\
            Analyze the commits and files to extract meaningful changes. Group them appropriately by type. \
            Include commit hashes, calculate metrics, and follow the exact JSON structure specified."
        );

        // Generate JSON response
        let json_response = self
            .generate_with_backend(&system_prompt, &user_prompt)
            .await?;

        // Parse and format using original formatter
        self.format_changelog_response(&json_response)
    }

    /// Create the original changelog system prompt for consistency
    fn create_original_changelog_system_prompt(&self, _context: &AgentContext) -> String {
        "You are an AI assistant specialized in generating clear, concise, and informative changelogs for software projects. \
        Your task is to create a well-structured changelog based on the provided commit information and analysis. \
        The changelog should adhere to the Keep a Changelog 1.1.0 format (https://keepachangelog.com/en/1.1.0/).

        Work step-by-step and follow these guidelines exactly:

        1. Categorize changes into the following types: Added, Changed, Deprecated, Removed, Fixed, Security.
        2. Use present tense and imperative mood in change descriptions.
        3. Start each change entry with a capital letter and do not end with a period.
        4. Be concise but descriptive in change entries and ensure good grammar, capitalization, and punctuation.
        5. Include *short* commit hashes at the end of each entry.
        6. Focus on the impact and significance of the changes and omit trivial changes below the relevance threshold.
        7. Find commonalities and group related changes together under the appropriate category.
        8. List the most impactful changes first within each category.
        9. Mention associated issue numbers and pull request numbers when available.
        10. Clearly identify and explain any breaking changes.
        11. Avoid common clich√© words (like 'enhance', 'streamline', 'leverage', etc) and phrases.
        12. Do not speculate about the purpose of a change or add any information not directly supported by the context.
        13. Mention any changes to dependencies or build configurations under the appropriate category.
        14. Highlight changes that affect multiple parts of the codebase or have cross-cutting concerns.
        15. NO YAPPING!

        Your response must be a valid JSON object with the following structure:

        {
          \"version\": \"string or null\",
          \"release_date\": \"string or null\",
          \"sections\": {
            \"Added\": [{ \"description\": \"string\", \"commit_hashes\": [\"string\"], \"associated_issues\": [\"string\"], \"pull_request\": \"string or null\" }],
            \"Changed\": [...],
            \"Deprecated\": [...],
            \"Removed\": [...],
            \"Fixed\": [...],
            \"Security\": [...]
          },
          \"breaking_changes\": [{ \"description\": \"string\", \"commit_hash\": \"string\" }],
          \"metrics\": {
            \"total_commits\": number,
            \"files_changed\": number,
            \"insertions\": number,
            \"deletions\": number
          }
        }".to_string()
    }

    /// Format changelog JSON response using the original formatter style
    fn format_changelog_response(&self, json_response: &str) -> Result<String> {
        // Clean the JSON response (remove markdown code blocks if present)
        let clean_json = self.clean_json_response(json_response);

        // Parse the JSON response (with fallback for malformed JSON)
        let parsed_response: serde_json::Value = match serde_json::from_str(&clean_json) {
            Ok(json) => json,
            Err(_) => {
                // Try our robust JSON parsing as fallback
                self.parse_json_response::<serde_json::Value>(&clean_json)?
            }
        };

        // Format using the original style
        let mut formatted = String::new();

        // Add header (no colors in agent output for consistency)
        formatted.push_str("# Changelog\n\n");
        formatted
            .push_str("All notable changes to this project will be documented in this file.\n\n");
        formatted.push_str(
            "The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\n",
        );
        formatted.push_str("and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n");

        // Add version
        let version = parsed_response
            .get("version")
            .and_then(|v| v.as_str())
            .unwrap_or("Unreleased");
        formatted.push_str(&format!("## [{version}] - \n\n"));

        // Process sections in order
        let ordered_types = [
            "Added",
            "Changed",
            "Fixed",
            "Removed",
            "Deprecated",
            "Security",
        ];

        if let Some(sections) = parsed_response.get("sections").and_then(|s| s.as_object()) {
            for change_type in &ordered_types {
                if let Some(entries) = sections.get(*change_type).and_then(|e| e.as_array()) {
                    if !entries.is_empty() {
                        // Add emoji and section header
                        let emoji = match *change_type {
                            "Added" => "‚ú®",
                            "Changed" => "üîÑ",
                            "Fixed" => "üêõ",
                            "Removed" => "üóëÔ∏è",
                            "Deprecated" => "‚ö†Ô∏è",
                            "Security" => "üîí",
                            _ => "üìù",
                        };
                        formatted.push_str(&format!("### {emoji} {change_type}\n\n"));

                        // Add entries
                        for entry in entries {
                            if let Some(description) =
                                entry.get("description").and_then(|d| d.as_str())
                            {
                                formatted.push_str(&format!("- {description}"));

                                // Add commit hashes
                                if let Some(hashes) =
                                    entry.get("commit_hashes").and_then(|h| h.as_array())
                                {
                                    let hash_strs: Vec<String> = hashes
                                        .iter()
                                        .filter_map(|h| h.as_str())
                                        .map(std::string::ToString::to_string)
                                        .collect();
                                    if !hash_strs.is_empty() {
                                        formatted.push_str(&format!(" ({})", hash_strs.join(", ")));
                                    }
                                }
                                formatted.push('\n');
                            }
                        }
                        formatted.push('\n');
                    }
                }
            }
        }

        // Add metrics
        if let Some(metrics) = parsed_response.get("metrics").and_then(|m| m.as_object()) {
            formatted.push_str("### üìä Metrics\n\n");
            if let Some(commits) = metrics
                .get("total_commits")
                .and_then(serde_json::Value::as_u64)
            {
                formatted.push_str(&format!("- Total Commits: {commits}\n"));
            }
            if let Some(files) = metrics
                .get("files_changed")
                .and_then(serde_json::Value::as_u64)
            {
                formatted.push_str(&format!("- Files Changed: {files}\n"));
            }
            if let Some(insertions) = metrics
                .get("insertions")
                .and_then(serde_json::Value::as_u64)
            {
                formatted.push_str(&format!("- Insertions: {insertions}\n"));
            }
            if let Some(deletions) = metrics.get("deletions").and_then(serde_json::Value::as_u64) {
                formatted.push_str(&format!("- Deletions: {deletions}\n"));
            }
        }

        Ok(formatted)
    }

    /// Add decorative borders like the original changelog output
    fn add_changelog_borders(&self, content: &str) -> String {
        let border = "‚îÅ".repeat(50);
        format!("{border}\n{content}\n{border}")
    }

    /// Clean JSON response by removing markdown code blocks
    fn clean_json_response(&self, response: &str) -> String {
        let response = response.trim();

        // Remove markdown code blocks if present
        if response.starts_with("```json") && response.ends_with("```") {
            // Remove ```json from start and ``` from end
            let without_start = response.strip_prefix("```json").unwrap_or(response);
            let without_end = without_start.strip_suffix("```").unwrap_or(without_start);
            without_end.trim().to_string()
        } else if response.starts_with("```") && response.ends_with("```") {
            // Remove generic ``` code blocks
            let without_start = response.strip_prefix("```").unwrap_or(response);
            let without_end = without_start.strip_suffix("```").unwrap_or(without_start);
            without_end.trim().to_string()
        } else {
            response.to_string()
        }
    }

    /// Generate release notes content using LLM with original format
    async fn generate_release_notes_content(
        &self,
        _context: &AgentContext,
        git_context: &str,
        version: &str,
    ) -> Result<String> {
        // Use the original release notes system prompt for consistency
        let system_prompt = self.create_original_release_notes_system_prompt();

        let user_prompt = format!(
            "Generate a structured release notes JSON for version {version} based on the following git context:\n\n{git_context}\n\n\
            Analyze the commits and files to extract meaningful changes. Follow the exact JSON structure specified. \
            Focus on technical accuracy and use the same tone as the changelog format."
        );

        // Generate JSON response
        let json_response = self
            .generate_with_backend(&system_prompt, &user_prompt)
            .await?;

        // Format using original release notes formatter style
        self.format_release_notes_response(&json_response, version)
    }

    /// Create the original release notes system prompt for consistency
    fn create_original_release_notes_system_prompt(&self) -> String {
        "You are an AI assistant specialized in generating technical release notes for software projects. \
        Your task is to create well-structured, professional release notes based on the provided commit information and analysis. \
        The release notes should be technical, factual, and follow a structured format.

        Work step-by-step and follow these guidelines exactly:

        1. Categorize changes into sections: Highlights, Added, Changed, Fixed, Refactored, Upgrade Notes.
        2. Use present tense and imperative mood in change descriptions.
        3. Be concise but descriptive and ensure good grammar, capitalization, and punctuation.
        4. Include *short* commit hashes at the end of each entry.
        5. Focus on the technical impact and significance of the changes.
        6. Group related changes together under appropriate sections.
        7. List the most impactful changes first within each section.
        8. Be factual and professional - avoid marketing language.
        9. Include metrics and statistics when available.
        10. Highlight any breaking changes or upgrade considerations.

        Your response must be a valid JSON object with the following structure:

        {
          \"version\": \"string\",
          \"release_date\": \"string or null\",
          \"summary\": \"string\",
          \"highlights\": [{ \"title\": \"string\", \"description\": \"string\", \"commit_hashes\": [\"string\"] }],
          \"sections\": {
            \"Added\": [{ \"description\": \"string\", \"commit_hashes\": [\"string\"] }],
            \"Changed\": [...],
            \"Fixed\": [...],
            \"Refactored\": [...]
          },
          \"upgrade_notes\": [\"string\"],
          \"metrics\": {
            \"total_commits\": number,
            \"files_changed\": number,
            \"insertions\": number,
            \"deletions\": number
          }
        }".to_string()
    }

    /// Format release notes JSON response using the original formatter style
    fn format_release_notes_response(&self, json_response: &str, version: &str) -> Result<String> {
        // Clean the JSON response (remove markdown code blocks if present)
        let clean_json = self.clean_json_response(json_response);

        // Parse the JSON response
        let parsed_response: serde_json::Value = match serde_json::from_str(&clean_json) {
            Ok(json) => json,
            Err(_) => {
                // Try our robust JSON parsing as fallback
                self.parse_json_response::<serde_json::Value>(&clean_json)?
            }
        };

        let mut formatted = String::new();

        // Add header
        formatted.push_str(&format!("# Release Notes - v{version}\n\n"));
        formatted.push_str("Release Date: \n\n");

        // Add summary
        if let Some(summary) = parsed_response.get("summary").and_then(|s| s.as_str()) {
            formatted.push_str(&format!("{summary}\n\n"));
        }

        // Add highlights section
        if let Some(highlights) = parsed_response.get("highlights").and_then(|h| h.as_array()) {
            if !highlights.is_empty() {
                formatted.push_str("## ‚ú® Highlights\n\n");
                for highlight in highlights {
                    if let (Some(title), Some(description)) = (
                        highlight.get("title").and_then(|t| t.as_str()),
                        highlight.get("description").and_then(|d| d.as_str()),
                    ) {
                        formatted.push_str(&format!("### {title}\n\n{description}\n\n"));
                    }
                }
            }
        }

        // Add sections in order
        let ordered_types = ["Added", "Changed", "Fixed", "Refactored"];

        if let Some(sections) = parsed_response.get("sections").and_then(|s| s.as_object()) {
            for change_type in &ordered_types {
                if let Some(entries) = sections.get(*change_type).and_then(|e| e.as_array()) {
                    if !entries.is_empty() {
                        // Add emoji and section header
                        let emoji = match *change_type {
                            "Added" => "‚ú®",
                            "Changed" => "üîß",
                            "Fixed" => "üêõ",
                            "Refactored" => "‚ôæÔ∏è",
                            _ => "üìù",
                        };
                        formatted.push_str(&format!("## {emoji} {change_type}\n\n"));

                        // Add entries
                        for entry in entries {
                            if let Some(description) =
                                entry.get("description").and_then(|d| d.as_str())
                            {
                                formatted.push_str(&format!("- {description}"));

                                // Add commit hashes
                                if let Some(hashes) =
                                    entry.get("commit_hashes").and_then(|h| h.as_array())
                                {
                                    let hash_strs: Vec<String> = hashes
                                        .iter()
                                        .filter_map(|h| h.as_str())
                                        .map(std::string::ToString::to_string)
                                        .collect();
                                    if !hash_strs.is_empty() {
                                        formatted.push_str(&format!(" ({})", hash_strs.join(", ")));
                                    }
                                }
                                formatted.push('\n');
                            }
                        }
                        formatted.push('\n');
                    }
                }
            }
        }

        // Add upgrade notes
        if let Some(upgrade_notes) = parsed_response
            .get("upgrade_notes")
            .and_then(|u| u.as_array())
        {
            if !upgrade_notes.is_empty() {
                formatted.push_str("## üîß Upgrade Notes\n\n");
                for note in upgrade_notes {
                    if let Some(note_str) = note.as_str() {
                        formatted.push_str(&format!("- {note_str}\n"));
                    }
                }
                formatted.push('\n');
            }
        }

        // Add metrics
        if let Some(metrics) = parsed_response.get("metrics").and_then(|m| m.as_object()) {
            formatted.push_str("## üìä Metrics\n\n");
            if let Some(commits) = metrics
                .get("total_commits")
                .and_then(serde_json::Value::as_u64)
            {
                formatted.push_str(&format!("- Total Commits: {commits}\n"));
            }
            if let Some(files) = metrics
                .get("files_changed")
                .and_then(serde_json::Value::as_u64)
            {
                formatted.push_str(&format!("- Files Changed: {files}\n"));
            }
            if let Some(insertions) = metrics
                .get("insertions")
                .and_then(serde_json::Value::as_u64)
            {
                formatted.push_str(&format!("- Insertions: {insertions}\n"));
            }
            if let Some(deletions) = metrics.get("deletions").and_then(serde_json::Value::as_u64) {
                formatted.push_str(&format!("- Deletions: {deletions}\n"));
            }
        }

        Ok(formatted)
    }

    /// Update changelog file with sophisticated file management
    async fn update_changelog_file(
        &self,
        changelog_content: &str,
        file_path: &str,
        context: &AgentContext,
        to_ref: Option<&str>,
        version_name: Option<String>,
    ) -> Result<()> {
        log_debug!("üìù Iris: Updating changelog file: {}", file_path);

        // Get commit date for version header
        let commit_date = if let Some(to_ref) = to_ref {
            context
                .git_repo
                .get_commit_date(to_ref)
                .unwrap_or_else(|_| chrono::Local::now().format("%Y-%m-%d").to_string())
        } else {
            chrono::Local::now().format("%Y-%m-%d").to_string()
        };

        // Default changelog header
        let default_header = "# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n";

        // Strip ANSI codes and clean content
        let clean_content = self.strip_ansi_codes(changelog_content);
        let mut version_content = self.extract_version_content(&clean_content);

        // Override version if provided
        if let Some(version) = version_name {
            version_content = self.replace_version_in_content(&version_content, &version);
        }

        // Add date to version header
        version_content = self.add_date_to_version(&version_content, &commit_date);

        // Read existing file or create new one
        let existing_content = tokio::fs::read_to_string(file_path)
            .await
            .unwrap_or_default();

        let final_content = if existing_content.is_empty() {
            format!("{default_header}{version_content}")
        } else {
            self.merge_with_existing_changelog(&existing_content, &version_content, default_header)
        };

        // Write updated content
        tokio::fs::write(file_path, final_content).await?;
        log_debug!("‚úÖ Iris: Changelog file updated successfully");

        Ok(())
    }

    /// Strip ANSI color codes from text
    fn strip_ansi_codes(&self, text: &str) -> String {
        // Simple ANSI escape sequence removal
        let re = Regex::new(r"\x1b\[[0-9;]*m").unwrap();
        re.replace_all(text, "").to_string()
    }

    /// Extract version content from changelog
    fn extract_version_content(&self, content: &str) -> String {
        // Skip separator lines and extract version content
        let clean_content = if content.starts_with("‚îÅ") || content.starts_with('-') {
            if let Some(pos) = content.find('\n') {
                content[pos + 1..].to_string()
            } else {
                content.to_string()
            }
        } else {
            content.to_string()
        };

        // Extract version section
        if clean_content.contains("## [") {
            let parts: Vec<&str> = clean_content.split("## [").collect();
            if parts.len() > 1 {
                format!("## [{}\n", parts[1])
            } else {
                clean_content
            }
        } else {
            clean_content
        }
    }

    /// Replace version in content
    fn replace_version_in_content(&self, content: &str, version: &str) -> String {
        let re = Regex::new(r"## \[([^\]]+)\]").unwrap();
        re.replace(content, &format!("## [{version}]")).to_string()
    }

    /// Add date to version header
    fn add_date_to_version(&self, content: &str, date: &str) -> String {
        if content.contains(" - \n") {
            content.replace(" - \n", &format!(" - {date}\n"))
        } else if content.contains("] - ") && !content.contains("] - 20") {
            let parts: Vec<&str> = content.splitn(2, "] - ").collect();
            if parts.len() == 2 {
                format!(
                    "{}] - {}\n{}",
                    parts[0],
                    date,
                    parts[1].trim_start_matches(['\n', ' '])
                )
            } else {
                content.to_string()
            }
        } else {
            content.to_string()
        }
    }

    /// Merge new content with existing changelog
    fn merge_with_existing_changelog(
        &self,
        existing: &str,
        new_version: &str,
        default_header: &str,
    ) -> String {
        let mut lines = existing.lines().collect::<Vec<_>>();
        let mut header_end = 0;

        // Find where header ends (first version entry)
        for (i, line) in lines.iter().enumerate() {
            if line.starts_with("## [") {
                header_end = i;
                break;
            }
        }

        // If no existing versions found, add after header
        if header_end == 0 {
            if lines.is_empty() {
                format!("{default_header}{new_version}")
            } else {
                format!("{existing}\n\n{new_version}")
            }
        } else {
            // Insert new version at the top of versions list
            lines.insert(header_end, "");
            lines.insert(header_end, new_version.trim());
            lines.join("\n")
        }
    }
}

#[async_trait]
impl super::core::IrisAgent for IrisAgent {
    fn id(&self) -> &str {
        &self.id
    }

    fn name(&self) -> &str {
        &self.name
    }

    fn description(&self) -> &str {
        &self.description
    }

    fn capabilities(&self) -> Vec<String> {
        self.capabilities.clone()
    }

    async fn execute_task(
        &self,
        task: &str,
        context: &AgentContext,
        params: &HashMap<String, serde_json::Value>,
    ) -> Result<TaskResult> {
        if !self.initialized {
            log_debug!(
                "‚ùå Iris: Agent not initialized, cannot execute task: {}",
                task
            );
            return Err(anyhow::anyhow!("Iris agent not initialized"));
        }

        log_debug!("üéØ Iris: Starting task execution: {}", task);
        log_debug!("üìã Iris: Task parameters: {} keys", params.len());

        let start_time = std::time::Instant::now();

        let result = match task {
            "generate_commit_message" | "commit_message_generation" => {
                log_debug!("üìù Iris: Executing commit message generation");
                self.generate_commit_message(context, params).await
            }
            "generate_code_review" | "code_review" | "review_code" => {
                log_debug!("üîç Iris: Executing code review generation");
                self.generate_code_review(context, params).await
            }
            "generate_pull_request" | "pull_request_description" => {
                log_debug!("üìã Iris: Executing pull request description generation");
                self.generate_pull_request(context, params).await
            }
            "changelog_generation" | "generate_changelog" => {
                log_debug!("üìú Iris: Executing changelog generation");
                self.generate_changelog(context, params).await
            }
            "generate_release_notes" | "release_notes" => {
                log_debug!("üéâ Iris: Executing release notes generation");
                self.generate_release_notes(context, params).await
            }
            _ => {
                log_debug!("‚ùå Iris: Unknown task requested: {}", task);
                Err(anyhow::anyhow!("Unknown task for Iris: {}", task))
            }
        };

        let duration = start_time.elapsed();

        match &result {
            Ok(task_result) => {
                log_debug!(
                    "‚úÖ Iris: Task '{}' completed successfully in {:.2}s (confidence: {:.2})",
                    task,
                    duration.as_secs_f64(),
                    task_result.confidence
                );
            }
            Err(e) => {
                log_debug!(
                    "‚ùå Iris: Task '{}' failed after {:.2}s: {}",
                    task,
                    duration.as_secs_f64(),
                    e
                );
            }
        }

        result
    }

    fn can_handle_task(&self, task: &str) -> bool {
        matches!(
            task,
            "generate_commit_message"
                | "commit_message_generation"
                | "generate_code_review"
                | "code_review"
                | "review_code"
                | "generate_pull_request"
                | "pull_request_description"
                | "changelog_generation"
                | "generate_changelog"
                | "release_notes_generation"
                | "generate_release_notes"
                | "file_analysis"
                | "relevance_scoring"
        )
    }

    fn task_priority(&self, task: &str) -> u8 {
        match task {
            "generate_commit_message"
            | "commit_message_generation"
            | "generate_code_review"
            | "code_review"
            | "review_code"
            | "generate_pull_request"
            | "pull_request_description" => 10,
            "changelog_generation"
            | "generate_changelog"
            | "release_notes_generation"
            | "generate_release_notes" => 9,
            "file_analysis" | "relevance_scoring" => 8,
            _ => 0,
        }
    }

    async fn initialize(&mut self, _context: &AgentContext) -> Result<()> {
        self.initialized = true;
        log_debug!("ü§ñ Iris: Agent initialized successfully");
        Ok(())
    }

    async fn cleanup(&self) -> Result<()> {
        log_debug!("ü§ñ Iris: Agent cleanup completed");
        Ok(())
    }

    fn as_any(&self) -> &dyn std::any::Any {
        self
    }
}
